{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the following article is to obtain a step-by-step guide on building the `Prediction intervals in forecasting models` using `Mlforecast`.\n",
    "\n",
    "During this walkthrough, we will become familiar with the main `MlForecast` class and some relevant methods such as `mlforecast.fit`, `mlforecast.predict` and `mlforecast.cross_validation` in other.\n",
    "\n",
    "Let's start!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "\n",
    "\n",
    "\n",
    "1.\t[Introduction](#1)\n",
    "2.\t[Prediction intervals in forecasting Model](#2)\n",
    "3.\t[Installing Mlforecast](#3)\n",
    "4.\t[Loading libraries and data](#4)\n",
    "5.\t[Explore Data with the plot method](#5)\n",
    "6.\t[Split the data into training and testing](#6)\n",
    "7.\t[Implementation of model with MLForecast](#7)\n",
    "8. [References](#8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Table of Contents](#0)\n",
    "\n",
    "The target of our prediction is something unknown (otherwise we wouldn't be making a prediction), so we can think of it as a random variable. For example, the total sales for the next month could have different possible values, and we won't know what the exact value will be until we get the actual sales at the end of the month. Until next month's sales are known, this is a random amount.\n",
    "\n",
    "By the time the next month draws near, we usually have a pretty good idea of possible sales values. However, if we are forecasting sales for the same month next year, the possible values can vary much more. In most forecasting cases, the variability associated with what we are forecasting reduces as we get closer to the event. In other words, the further back in time we make the prediction, the more uncertainty there is.\n",
    "\n",
    "We can imagine many possible future scenarios, each yielding a different value for what we are trying to forecast.\n",
    "\n",
    "When we obtain a forecast, we are estimating the middle of the range of possible values the random variable could take. Often, a forecast is accompanied by a prediction interval giving a range of values the random variable could take with relatively high probability. For example, a 95% prediction interval contains a range of values which should include the actual future value with probability 95%.\n",
    "\n",
    "Rather than plotting individual possible futures , we usually show these prediction intervals instead.\n",
    "\n",
    "When we generate a forecast, we usually produce a single value known as the point forecast. This value, however, doesn’t tell us anything about the uncertainty associated with the forecast. To have a measure of this uncertainty, we need prediction intervals.\n",
    "\n",
    "A prediction interval is a range of values that the forecast can take with a given probability. Hence, a 95% prediction interval should contain a range of values that include the actual future value with probability 95%. Probabilistic forecasting aims to generate the full forecast distribution. Point forecasting, on the other hand, usually returns the mean or the median or said distribution. However, in real-world scenarios, it is better to forecast not only the most probable future outcome, but many alternative outcomes as well.\n",
    "\n",
    "The problem is that some timeseries models provide forecast distributions, but some other ones only provide point forecasts. How can we then estimate the uncertainty of predictions?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Forecasts and prediction intervals** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Table of Contents](#0)\n",
    "\n",
    "There are at least four sources of uncertainty in forecasting using time series models:\n",
    "\n",
    "1. The random error term;\n",
    "2. The parameter estimates;\n",
    "3. The choice of model for the historical data;\n",
    "4. The continuation of the historical data generating process into the future.\n",
    "\n",
    "When we produce prediction intervals for time series models, we generally only take into account the first of these sources of uncertainty. It would be possible to account for 2 and 3 using simulations, but that is almost never done because it would take too much time to compute. As computing speeds increase, it might become a viable approach in the future.\n",
    "\n",
    "Even if we ignore the model uncertainty and the DGP uncertainty (sources 3 and 4), and just try to allow for parameter uncertainty as well as the random error term (sources 1 and 2), there are no closed form solutions apart from some simple special cases. see full article [Rob J Hyndman](https://robjhyndman.com/hyndsight/narrow-pi/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Forecast distributions**\n",
    "We use forecast distributions to express the uncertainty in our predictions. These probability distributions describe the probability of observing different future values using the fitted model. The point forecast corresponds to the mean of this distribution. Most time series models generate forecasts that follow a normal distribution, which implies that we assume that possible future values follow a normal distribution. However, later in this section we will look at some alternatives to normal distributions.\n",
    "\n",
    "### **Importance of Confidence Interval Prediction in Time Series:**\n",
    "\n",
    "1. Uncertainty Estimation: The confidence interval provides a measure of the uncertainty associated with time series predictions. It enables variability and the range of possible future values to be quantified, which is essential for making informed decisions.\n",
    "\n",
    "2. Precision evaluation: By having a confidence interval, the precision of the predictions can be evaluated. If the interval is narrow, it indicates that the forecast is more accurate and reliable. On the other hand, if the interval is wide, it indicates greater uncertainty and less precision in the predictions.\n",
    "\n",
    "3. Risk management: The confidence interval helps in risk management by providing information about possible future scenarios. It allows identifying the ranges in which the real values could be located and making decisions based on those possible scenarios.\n",
    "\n",
    "4. Effective communication: The confidence interval is a useful tool for communicating predictions clearly and accurately. It allows the variability and uncertainty associated with the predictions to be conveyed to the stakeholders, avoiding a wrong or overly optimistic interpretation of the results.\n",
    "\n",
    "Therefore, confidence interval prediction in time series is essential to understand and manage uncertainty, assess the accuracy of predictions, and make informed decisions based on possible future scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Prediction intervals**\n",
    "\n",
    "A prediction interval gives us a range in which we expect $y_t$ to lie with a specified probability. For example, if we assume that the distribution of future observations follows a normal distribution, a 95% prediction interval for the forecast of step h would be represented by the range \n",
    "\n",
    "$$\\hat{y}_{T+h|T} \\pm 1.96 \\hat\\sigma_h,$$\n",
    "\n",
    "Where $\\hat\\sigma_h$ is an estimate of the standard deviation of the h -step forecast distribution.\n",
    "\n",
    "More generally, a prediction interval can be written as\n",
    "\n",
    "$$\\hat{y}_{T+h|T} \\pm c \\hat\\sigma_h$$\n",
    "\n",
    "In this context, the term \"multiplier c\" is associated with the probability of coverage. In this article, intervals of 80% and 95% are typically calculated, but any other percentage can be used. The table below shows the values of c corresponding to different coverage probabilities, assuming a normal forecast distribution.\n",
    "\n",
    "|Percentage\t|Multiplier|\n",
    "|-----------|----------|\n",
    "|50|0.67|\n",
    "|55\t|0.76|\n",
    "|60\t|0.84|\n",
    "|65\t|0.93|\n",
    "|70\t|1.04|\n",
    "|75\t|1.15|\n",
    "|80\t|1.28|\n",
    "|85\t|1.44|\n",
    "|90\t|1.64|\n",
    "|95\t|1.96|\n",
    "|96\t|2.05|\n",
    "|97\t|2.17|\n",
    "|98\t|2.33|\n",
    "|99\t|2.58|\n",
    "\n",
    "Prediction intervals are valuable because they reflect the uncertainty in the predictions. If we only generate point forecasts, we cannot assess how accurate those forecasts are. However, by providing prediction intervals, the amount of uncertainty associated with each forecast becomes apparent. For this reason, point forecasts may lack significant value without the inclusion of corresponding forecast intervals.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 One-step prediction intervals**\n",
    "\n",
    "When making a prediction for a future step, it is possible to estimate the standard deviation of the forecast distribution using the standard deviation of the residuals, which is calculated by\n",
    "\n",
    "\\begin{equation}\n",
    "  \\hat{\\sigma} = \\sqrt{\\frac{1}{T-K-M}\\sum_{t=1}^T e_t^2}, \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "where $K$ is the number of parameters estimated in the forecasting method, and $M$ is the number of missing values in the residuals. (For example, $M=1$ for a naive forecast, because we can’t forecast the first observation.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4. Multi-step prediction intervals** \n",
    "\n",
    "A typical feature of forecast intervals is that they tend to increase in length as the forecast horizon lengthens. As we move further out in time, there is greater uncertainty associated with the prediction, resulting in wider prediction intervals. In general, σh tends to increase as h increases (although there are some nonlinear forecasting methods that do not follow this property).\n",
    "\n",
    "To generate a prediction interval, it is necessary to have an estimate of σh. As mentioned above, for one-step forecasts (h=1), equation (1) provides a good estimate of the standard deviation of the forecast, σ1. However, for multi-step forecasts, a more complex calculation method is required. These calculations assume that the residuals are uncorrelated with each other.\n",
    "\n",
    "## Benchmark methods\n",
    "For the four benchmark methods, it is possible to mathematically derive the forecast standard deviation under the assumption of uncorrelated residuals. If $\\hat{\\sigma}_h$ denotes the standard deviation of the $h$ -step forecast distribution, and $\\hat{\\sigma}$ is the residual standard deviation given by (1), then we can use the expressions shown in next Table. Note that when $h=1$ and  $T$ is large, these all give the same approximate value $\\hat{\\sigma}$.\n",
    "\n",
    "|Method|h-step forecast standard deviation|\n",
    "|------|----------------------------------|\n",
    "|Mean forecasts|$\\hat\\sigma_h = \\hat\\sigma\\sqrt{1 + 1/T}$|\n",
    "|Naïve forecasts|$\\hat\\sigma_h = \\hat\\sigma\\sqrt{h}$|\n",
    "|Seasonal naïve forecasts|$\\hat\\sigma_h = \\hat\\sigma\\sqrt{k+1}$|\n",
    "|Drift forecasts|$\\hat\\sigma_h = \\hat\\sigma\\sqrt{h(1+h/T)}$|\n",
    "\n",
    "Note that when $h=1$ and $T$ is large, these all give the same approximate value $\\hat{\\sigma}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.5 Prediction intervals from bootstrapped residuals**\n",
    "\n",
    "When a normal distribution for the residuals is an unreasonable assumption, one alternative is to use bootstrapping, which only assumes that the residuals are uncorrelated with constant variance. We will illustrate the procedure using a naïve forecasting method.\n",
    "\n",
    "A one-step forecast error is defined as $e_t = y_t - \\hat{y}_{t|t-1}$. For a naïve forecasting method, \\hat{y}_{t|t-1} = y_{t-1}, so we can rewrite this as\n",
    "$$y_t = y_{t-1} + e_t.$$\n",
    "\n",
    "Assuming future errors will be similar to past errors, when $t>T$ we can replace $e_{t}$ by sampling from the collection of errors we have seen in the past (i.e., the residuals). So we can simulate the next observation of a time series using\n",
    "\n",
    "$$y^*_{T+1} = y_{T} + e^*_{T+1}$$\n",
    "\n",
    "where $e^*_{T+1}$ is a randomly sampled error from the past, and $y^*_{T+1}$  is the possible future value that would arise if that particular error value occurred. We use We use a * to indicate that this is not the observed $y_{T+1}$ value, but one possible future that could occur. Adding the new simulated observation to our data set, we can repeat the process to obtain \n",
    "\n",
    "$$y^*_{T+2} = y_{T+1}^* + e^*_{T+2},$$\n",
    "\n",
    "where $e^*_{T+2}$ is another draw from the collection of residuals. Continuing in this way, we can simulate an entire set of future values for our time series."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.6 Conformal Prediction**\n",
    "\n",
    "Multi-quantile losses and statistical models can provide provide prediction intervals, but the problem is that these are uncalibrated, meaning that the actual frequency of observations falling within the interval does not align with the confidence level associated with it. For example, a calibrated 95% prediction interval should contain the true value 95% of the time in repeated sampling. An uncalibrated 95% prediction interval, on the other hand, might contain the true value only 80% of the time, or perhaps 99% of the time. In the first case, the interval is too narrow and underestimates the uncertainty, while in the second case, it is too wide and overestimates the uncertainty.\n",
    "\n",
    "Statistical methods also assume normality. Here, we talk about another method called conformal prediction that doesn’t require any distributional assumptions.\n",
    "\n",
    "Conformal prediction intervals use cross-validation on a point forecaster model to generate the intervals. This means that no prior probabilities are needed, and the output is well-calibrated. No additional training is needed, and the model is treated as a black box. The approach is compatible with any model\n",
    "\n",
    "[MlForecast](https://github.com/nixtla/mlforecast) now supports Conformal Prediction on all available models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Installing Mlforecast** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "* using pip:\n",
    "\n",
    "    - `pip install mlforecast`\n",
    "\n",
    "* Specific version\n",
    "\n",
    "    If you want a specific version you can include a filter, for example:\n",
    "\n",
    "    - `pip install \"mlforecast==0.3.0\"` to install the 0.3.0 version\n",
    "    - `pip install \"mlforecast<0.4.0\"` to install any version prior to 0.4.0\n",
    "\n",
    "* using with conda:\n",
    "\n",
    "    - `conda install -c conda-forge mlforecast`\n",
    "\n",
    "* Specific version\n",
    "\n",
    "    If you want a specific version you can include a filter, for example: \n",
    "\n",
    "    - `conda install -c conda-forge \"mlforecast==0.3.0\"` to install the 0.3.0 version\n",
    "    - `conda install -c conda-forge \"mlforecast<0.4.0\"` to install any version prior to 0.4.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Loading libraries and data** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling and processing of Data\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Handling and processing of Data for Date (time)\n",
    "# ==============================================================================\n",
    "import datetime\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# \n",
    "# ==============================================================================\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "# \n",
    "# ==============================================================================\n",
    "from utilsforecast.plotting import plot_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "import xgboost as xgb\n",
    "\n",
    "# \n",
    "# ==============================================================================\n",
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "from window_ops.ewm import ewm_mean\n",
    "from mlforecast.target_transforms import Differences\n",
    "\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plt.style.use('grayscale') # fivethirtyeight  grayscale  classic\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "dark_style = {\n",
    "    'figure.facecolor': '#008080',  # #212946\n",
    "    'axes.facecolor': '#008080',\n",
    "    'savefig.facecolor': '#008080',\n",
    "    'axes.grid': True,\n",
    "    'axes.grid.which': 'both',\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'grid.color': '#000000',  #2A3459\n",
    "    'grid.linewidth': '1',\n",
    "    'text.color': '0.9',\n",
    "    'axes.labelcolor': '0.9',\n",
    "    'xtick.color': '0.9',\n",
    "    'ytick.color': '0.9',\n",
    "    'font.size': 12 }\n",
    "plt.rcParams.update(dark_style)\n",
    "# Define the plot size\n",
    "# ==============================================================================\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18,7)\n",
    "\n",
    "# Hide warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-07-01 00:00:00</td>\n",
       "      <td>10844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 00:30:00</td>\n",
       "      <td>8127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-01 01:00:00</td>\n",
       "      <td>6210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-07-01 01:30:00</td>\n",
       "      <td>4656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-07-01 02:00:00</td>\n",
       "      <td>3820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  value\n",
       "0 2014-07-01 00:00:00  10844\n",
       "1 2014-07-01 00:30:00   8127\n",
       "2 2014-07-01 01:00:00   6210\n",
       "3 2014-07-01 01:30:00   4656\n",
       "4 2014-07-01 02:00:00   3820"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/nyc_taxi.csv\",parse_dates=[\"timestamp\"] )\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to MlForecast is always a data frame in long format with three columns: unique_id, ds and y:\n",
    "\n",
    "* The `unique_id` (string, int or category) represents an identifier for the series.\n",
    "\n",
    "* The `ds` (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n",
    "\n",
    "* The `y` (numeric) represents the measurement we wish to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-07-01 00:00:00</td>\n",
       "      <td>10844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 00:30:00</td>\n",
       "      <td>8127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-01 01:00:00</td>\n",
       "      <td>6210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-07-01 01:30:00</td>\n",
       "      <td>4656</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-07-01 02:00:00</td>\n",
       "      <td>3820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ds      y unique_id\n",
       "0 2014-07-01 00:00:00  10844         1\n",
       "1 2014-07-01 00:30:00   8127         1\n",
       "2 2014-07-01 01:00:00   6210         1\n",
       "3 2014-07-01 01:30:00   4656         1\n",
       "4 2014-07-01 02:00:00   3820         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"unique_id\"]=\"1\"\n",
    "df.columns=[\"ds\", \"y\", \"unique_id\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10320 entries, 0 to 10319\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   ds         10320 non-null  datetime64[ns]\n",
      " 1   y          10320 non-null  int64         \n",
      " 2   unique_id  10320 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(1)\n",
      "memory usage: 242.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Explore Data with the plot method** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Plot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_series(df)\n",
    "fig.savefig('../figs/prediction_intervals_in_forecasting_models__eda.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/prediction_intervals_in_forecasting_models__eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 The Augmented Dickey-Fuller Test**\n",
    "An Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\n",
    "\n",
    "Augmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\n",
    "\n",
    "- Null Hypothesis: Time Series is non-stationary. It gives a time-dependent trend.\n",
    "- Alternate Hypothesis: Time Series is stationary. In another term, the series doesn’t depend on time.\n",
    "\n",
    "- ADF or t Statistic < critical values: Reject the null hypothesis, time series is stationary.\n",
    "- ADF or t Statistic > critical values: Failed to reject the null hypothesis, time series is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_dickey_fuller_test(series , column_name):\n",
    "    print (f'Dickey-Fuller test results for columns: {column_name}')\n",
    "    dftest = adfuller(series, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n",
    "    for key,value in dftest[4].items():\n",
    "       dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "    if dftest[1] <= 0.05:\n",
    "        print(\"Conclusion:====>\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"The data is stationary\")\n",
    "    else:\n",
    "        print(\"Conclusion:====>\")\n",
    "        print(\"The null hypothesis cannot be rejected\")\n",
    "        print(\"The data is not stationary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dickey-Fuller test results for columns: Ads\n",
      "Test Statistic                -1.076452e+01\n",
      "p-value                        2.472132e-19\n",
      "No Lags Used                   3.900000e+01\n",
      "Number of observations used    1.028000e+04\n",
      "Critical Value (1%)           -3.430986e+00\n",
      "Critical Value (5%)           -2.861821e+00\n",
      "Critical Value (10%)          -2.566920e+00\n",
      "dtype: float64\n",
      "Conclusion:====>\n",
      "Reject the null hypothesis\n",
      "The data is stationary\n"
     ]
    }
   ],
   "source": [
    "augmented_dickey_fuller_test(df[\"y\"],'Ads')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 Autocorrelation plots**\n",
    "\n",
    "### **Autocorrelation Function**\n",
    "\n",
    "**Definition 1.** Let $\\{x_t;1 ≤ t ≤ n\\}$ be a time series sample of size n from $\\{X_t\\}$.\n",
    "1. $\\bar x = \\sum_{t=1}^n \\frac{x_t}{n}$ is called the sample mean of $\\{X_t\\}$.\n",
    "2. $c_k =\\sum_{t=1}^{n−k} (x_{t+k}- \\bar x)(x_t−\\bar x)/n$ is known as the sample autocovariance function of $\\{X_t\\}$.\n",
    "3. $r_k = c_k /c_0$ is said to be the sample autocorrelation function of $\\{X_t\\}$. \n",
    "\n",
    "Note the following remarks about this definition:\n",
    " \n",
    "* Like most literature, this guide uses ACF to denote the sample autocorrelation function as well as the autocorrelation function. What is denoted by ACF can easily be identified in context.\n",
    "\n",
    "* Clearly c0 is the sample variance of $\\{X_t\\}$. Besides, $r_0 = c_0/c_0 = 1$ and for any integer $k, |r_k| ≤ 1$.\n",
    "\n",
    "* When we compute the ACF of any sample series with a fixed length $n$, we cannot put too much confidence in the values of $r_k$ for large k’s, since fewer pairs of $(x_{t +k }, x_t )$ are available for calculating $r_k$ as $k$ is large. One rule of thumb is not to estimate $r_k$ for $k > n/3$, and another is $n ≥ 50, k ≤ n/4$. In any case, it is always a good idea to be careful.\n",
    "\n",
    "* We also compute the ACF of a nonstationary time series sample by Definition 1. In this case, however, the ACF or $r_k$ very slowly or hardly tapers off as $k$ increases.\n",
    "\n",
    "* Plotting the ACF $(r_k)$ against lag $k$ is easy but very helpful in analyzing time series sample. Such an ACF plot is known as a correlogram.\n",
    "\n",
    "* If $\\{X_t\\}$ is stationary with $E(X_t)=0$ and $\\rho_k =0$ for all $k \\neq 0$,thatis,itisa white noise series, then the sampling distribution of $r_k$ is asymptotically normal with the mean 0 and the variance of $1/n$. Hence, there is about 95% chance that $r_k$ falls in the interval $[−1.96/√n, 1.96/√n]$.\n",
    "\n",
    "Now we can give a summary that (1) if the time series plot of a time series clearly shows a trend or/and seasonality, it is surely nonstationary; (2) if the ACF $r_k$ very slowly or hardly tapers off as lag $k$ increases, the time series should also be nonstationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "plot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\n",
    "axs[0].set_title(\"Autocorrelation\");\n",
    "\n",
    "# Grafico\n",
    "plot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\n",
    "axs[1].set_title('Partial Autocorrelation')\n",
    "plt.savefig(\"../figs/prediction_intervals_in_forecasting_models__autocorrelation.png\")\n",
    "plt.close();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/prediction_intervals_in_forecasting_models__autocorrelation.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3 Decomposition of the time series**\n",
    "\n",
    "How to decompose a time series and why?\n",
    "\n",
    "In time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n",
    "\n",
    "* **Level:** This is the primary value that averages over time.\n",
    "* **Trend:** The trend is the value that causes increasing or decreasing patterns in a time series.\n",
    "* **Seasonality:** This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\n",
    "* **Residual/Noise:** These are the random variations in the time series.\n",
    "\n",
    "Combining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\n",
    "\n",
    "If seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\n",
    "\n",
    "The combination of the components in time series can be of two types:\n",
    "* Additive\n",
    "* multiplicative\n",
    "\n",
    "Additive time series\n",
    "\n",
    "If the components of the time series are added to make the time series. Then the time series is called the additive time series. By visualization, we can say that the time series is additive if the increasing or decreasing pattern of the time series is similar throughout the series. The mathematical function of any additive time series can be represented by:\n",
    "$$y(t) = level + Trend + seasonality + noise$$\n",
    "\n",
    "## Multiplicative time series\n",
    "\n",
    "If the components of the time series are multiplicative together, then the time series is called a multiplicative time series. For visualization, if the time series is having exponential growth or decline with time, then the time series can be considered as the multiplicative time series. The mathematical function of the multiplicative time series can be represented as.\n",
    "\n",
    "$$y(t) = Level * Trend * seasonality * Noise$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = seasonal_decompose(df[\"y\"], model = \"additive\", period=24).plot()\n",
    "a.savefig('../figs/prediction_intervals_in_forecasting_models__seasonal_decompose_aditive.png')\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/prediction_intervals_in_forecasting_models__seasonal_decompose_aditive.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=24).plot()\n",
    "b.savefig('../figs/prediction_intervals_in_forecasting_models__seasonal_decompose_multiplicative.png')\n",
    "plt.close();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/prediction_intervals_in_forecasting_models__seasonal_decompose_multiplicative.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Split the data into training and testing** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Let's divide our data into sets\n",
    "1. Data to train our model.\n",
    "2. Data to test our model\n",
    "\n",
    "For the test data we will use the last 500 hours to test and evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df.ds<='2015-01-21 13:30:00'] \n",
    "test = df[df.ds>'2015-01-21 13:30:00'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9820, 3), (500, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the training data and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_series(train,test)\n",
    "fig.savefig('../figs/prediction_intervals_in_forecasting_models__train_test.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/prediction_intervals_in_forecasting_models__train_test.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Modeling with MLForecast** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.1 Building Model**\n",
    "\n",
    "We define the model that we want to use, for our example we are going to use the `XGBoost model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = [xgb.XGBRegressor()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `MLForecast.preprocess` method to explore different transformations.\n",
    "\n",
    "If it is true that the series we are working with is a stationary series see (Dickey fuller test), however for the sake of practice and instruction in this guide, we will apply the difference to our series, we will do this using the `target_transforms parameter ` and calling the diff function like: `mlforecast.target_transforms.Differences`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the models by instantiating a new `MlForecast` object with the following parameters:\n",
    "\n",
    "* `models:` a list of models. Select the models you want from models and import them.\n",
    "\n",
    "* `freq:` a string indicating the frequency of the data. (See [panda’s available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).)\n",
    "\n",
    "* `lags:` Lags of the target to uses as feature.\n",
    "\n",
    "* `lag_transforms:` Mapping of target lags to their transformations.\n",
    "\n",
    "* `date_features:` Features computed from the dates. Can be `pandas` date attributes or functions that will take the dates as input.\n",
    "\n",
    "* `differences:` Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "\n",
    "* `num_threads:` Number of threads to use when computing the features.\n",
    "\n",
    "* `target_transforms:` Transformations that will be applied to the target computing the features and restored after the forecasting step.\n",
    "\n",
    "Any settings are passed into the constructor. Then you call its fit method and pass in the historical data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MLForecast(models=model1,\n",
    "                 freq='30min', \n",
    "                 target_transforms=[Differences([1])],\n",
    "                 ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to take into account when we use the parameter `target_transforms=[Differences([1])]` in case the series is stationary we can use a difference, or in the case that the series is not stationary, we can use more than one difference so that the series is constant over time, that is, that it is constant in mean and in variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 00:30:00</td>\n",
       "      <td>-2717.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-01 01:00:00</td>\n",
       "      <td>-1917.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-07-01 01:30:00</td>\n",
       "      <td>-1554.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-07-01 02:00:00</td>\n",
       "      <td>-836.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014-07-01 02:30:00</td>\n",
       "      <td>-947.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10315</th>\n",
       "      <td>2015-01-31 21:30:00</td>\n",
       "      <td>951.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10316</th>\n",
       "      <td>2015-01-31 22:00:00</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10317</th>\n",
       "      <td>2015-01-31 22:30:00</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10318</th>\n",
       "      <td>2015-01-31 23:00:00</td>\n",
       "      <td>-718.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10319</th>\n",
       "      <td>2015-01-31 23:30:00</td>\n",
       "      <td>-303.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10319 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds       y unique_id\n",
       "1     2014-07-01 00:30:00 -2717.0         1\n",
       "2     2014-07-01 01:00:00 -1917.0         1\n",
       "3     2014-07-01 01:30:00 -1554.0         1\n",
       "4     2014-07-01 02:00:00  -836.0         1\n",
       "5     2014-07-01 02:30:00  -947.0         1\n",
       "...                   ...     ...       ...\n",
       "10315 2015-01-31 21:30:00   951.0         1\n",
       "10316 2015-01-31 22:00:00  1051.0         1\n",
       "10317 2015-01-31 22:30:00  1588.0         1\n",
       "10318 2015-01-31 23:00:00  -718.0         1\n",
       "10319 2015-01-31 23:30:00  -303.0         1\n",
       "\n",
       "[10319 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = mlf.preprocess(df)\n",
    "prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has subtacted the lag 1 from each value, we can see what our series look like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plot_series(prep)\n",
    "fig.savefig('../figs/prediction_intervals_in_forecasting_models__plot_values.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/prediction_intervals_in_forecasting_models__plot_values.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.2 Adding features**\n",
    "\n",
    "### Lags\n",
    "Looks like the seasonality is gone, we can now try adding some lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MLForecast(models=model1,\n",
    "                 freq='30min',  \n",
    "                 lags=[1,24],\n",
    "                 target_transforms=[Differences([1])],\n",
    "                 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>lag1</th>\n",
       "      <th>lag24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2014-07-01 12:30:00</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>445.0</td>\n",
       "      <td>-2717.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2014-07-01 13:00:00</td>\n",
       "      <td>-708.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>-1917.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2014-07-01 13:30:00</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-708.0</td>\n",
       "      <td>-1554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2014-07-01 14:00:00</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>-836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2014-07-01 14:30:00</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>1</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-947.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10315</th>\n",
       "      <td>2015-01-31 21:30:00</td>\n",
       "      <td>951.0</td>\n",
       "      <td>1</td>\n",
       "      <td>428.0</td>\n",
       "      <td>4642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10316</th>\n",
       "      <td>2015-01-31 22:00:00</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951.0</td>\n",
       "      <td>-519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10317</th>\n",
       "      <td>2015-01-31 22:30:00</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>2411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10318</th>\n",
       "      <td>2015-01-31 23:00:00</td>\n",
       "      <td>-718.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10319</th>\n",
       "      <td>2015-01-31 23:30:00</td>\n",
       "      <td>-303.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-718.0</td>\n",
       "      <td>2595.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10295 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds       y unique_id    lag1   lag24\n",
       "25    2014-07-01 12:30:00   -22.0         1   445.0 -2717.0\n",
       "26    2014-07-01 13:00:00  -708.0         1   -22.0 -1917.0\n",
       "27    2014-07-01 13:30:00  1281.0         1  -708.0 -1554.0\n",
       "28    2014-07-01 14:00:00    87.0         1  1281.0  -836.0\n",
       "29    2014-07-01 14:30:00  1045.0         1    87.0  -947.0\n",
       "...                   ...     ...       ...     ...     ...\n",
       "10315 2015-01-31 21:30:00   951.0         1   428.0  4642.0\n",
       "10316 2015-01-31 22:00:00  1051.0         1   951.0  -519.0\n",
       "10317 2015-01-31 22:30:00  1588.0         1  1051.0  2411.0\n",
       "10318 2015-01-31 23:00:00  -718.0         1  1588.0   214.0\n",
       "10319 2015-01-31 23:30:00  -303.0         1  -718.0  2595.0\n",
       "\n",
       "[10295 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = mlf.preprocess(df)\n",
    "prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y        1.000000\n",
       "lag1     0.663082\n",
       "lag24    0.155366\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.drop(columns=['unique_id', 'ds']).corr()['y']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.3 Lag transforms**\n",
    "Lag transforms are defined as a dictionary where the keys are the lags and the values are lists of functions that transform an array. These must be [numba](http://numba.pydata.org) jitted functions (so that computing the features doesn’t become a bottleneck). There are some implemented in the [window-ops package](https://github.com/jmoralez/window_ops) but you can also implement your own.\n",
    "\n",
    "If the function takes two or more arguments you can either:\n",
    "\n",
    "* supply a tuple (tfm_func, arg1, arg2, …)\n",
    "* define a new function fixing the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MLForecast(models=model1,\n",
    "                 freq='30min',  \n",
    "                 lags=[1,24],\n",
    "                 lag_transforms={1: [expanding_mean],24: [(rolling_mean, 7)] },\n",
    "                 target_transforms=[Differences([1])],\n",
    "                 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = mlf.preprocess(df)\n",
    "prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that both approaches get to the same result, you can use whichever one you feel most comfortable with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.4 Date features**\n",
    "If your time column is made of timestamps then it might make sense to extract features like week, dayofweek, quarter, etc. You can do that by passing a list of strings with pandas time/date components. You can also pass functions that will take the time column as input, as we’ll show here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MLForecast(models=model1,\n",
    "                 freq='30min', \n",
    "                 lags=[1,24],\n",
    "                 lag_transforms={1: [expanding_mean],24: [(rolling_mean, 7)] },\n",
    "                 target_transforms=[Differences([1])],\n",
    "                 date_features=[\"year\", \"month\", \"day\", \"hour\"]) # Seasonal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>lag1</th>\n",
       "      <th>lag24</th>\n",
       "      <th>expanding_mean_lag1</th>\n",
       "      <th>rolling_mean_lag24_window_size7</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2014-07-01 15:30:00</td>\n",
       "      <td>-836.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1211.0</td>\n",
       "      <td>-305.0</td>\n",
       "      <td>284.533325</td>\n",
       "      <td>-1254.285767</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2014-07-01 16:00:00</td>\n",
       "      <td>-2316.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-836.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>248.387100</td>\n",
       "      <td>-843.714294</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2014-07-01 16:30:00</td>\n",
       "      <td>-1215.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2316.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>168.250000</td>\n",
       "      <td>-578.857117</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2014-07-01 17:00:00</td>\n",
       "      <td>2190.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1215.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>126.333336</td>\n",
       "      <td>-305.857147</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2014-07-01 17:30:00</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2190.0</td>\n",
       "      <td>1849.0</td>\n",
       "      <td>187.029419</td>\n",
       "      <td>77.714287</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10315</th>\n",
       "      <td>2015-01-31 21:30:00</td>\n",
       "      <td>951.0</td>\n",
       "      <td>1</td>\n",
       "      <td>428.0</td>\n",
       "      <td>4642.0</td>\n",
       "      <td>1.248303</td>\n",
       "      <td>2064.285645</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10316</th>\n",
       "      <td>2015-01-31 22:00:00</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951.0</td>\n",
       "      <td>-519.0</td>\n",
       "      <td>1.340378</td>\n",
       "      <td>1873.428589</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10317</th>\n",
       "      <td>2015-01-31 22:30:00</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>2411.0</td>\n",
       "      <td>1.442129</td>\n",
       "      <td>2179.000000</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10318</th>\n",
       "      <td>2015-01-31 23:00:00</td>\n",
       "      <td>-718.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>1.595910</td>\n",
       "      <td>1888.714233</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10319</th>\n",
       "      <td>2015-01-31 23:30:00</td>\n",
       "      <td>-303.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-718.0</td>\n",
       "      <td>2595.0</td>\n",
       "      <td>1.526168</td>\n",
       "      <td>2071.714355</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10289 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds       y unique_id    lag1   lag24  \\\n",
       "31    2014-07-01 15:30:00  -836.0         1 -1211.0  -305.0   \n",
       "32    2014-07-01 16:00:00 -2316.0         1  -836.0   157.0   \n",
       "33    2014-07-01 16:30:00 -1215.0         1 -2316.0   -63.0   \n",
       "34    2014-07-01 17:00:00  2190.0         1 -1215.0   357.0   \n",
       "35    2014-07-01 17:30:00  2322.0         1  2190.0  1849.0   \n",
       "...                   ...     ...       ...     ...     ...   \n",
       "10315 2015-01-31 21:30:00   951.0         1   428.0  4642.0   \n",
       "10316 2015-01-31 22:00:00  1051.0         1   951.0  -519.0   \n",
       "10317 2015-01-31 22:30:00  1588.0         1  1051.0  2411.0   \n",
       "10318 2015-01-31 23:00:00  -718.0         1  1588.0   214.0   \n",
       "10319 2015-01-31 23:30:00  -303.0         1  -718.0  2595.0   \n",
       "\n",
       "       expanding_mean_lag1  rolling_mean_lag24_window_size7  year  month  day  \\\n",
       "31              284.533325                     -1254.285767  2014      7    1   \n",
       "32              248.387100                      -843.714294  2014      7    1   \n",
       "33              168.250000                      -578.857117  2014      7    1   \n",
       "34              126.333336                      -305.857147  2014      7    1   \n",
       "35              187.029419                        77.714287  2014      7    1   \n",
       "...                    ...                              ...   ...    ...  ...   \n",
       "10315             1.248303                      2064.285645  2015      1   31   \n",
       "10316             1.340378                      1873.428589  2015      1   31   \n",
       "10317             1.442129                      2179.000000  2015      1   31   \n",
       "10318             1.595910                      1888.714233  2015      1   31   \n",
       "10319             1.526168                      2071.714355  2015      1   31   \n",
       "\n",
       "       hour  \n",
       "31       15  \n",
       "32       16  \n",
       "33       16  \n",
       "34       17  \n",
       "35       17  \n",
       "...     ...  \n",
       "10315    21  \n",
       "10316    22  \n",
       "10317    22  \n",
       "10318    23  \n",
       "10319    23  \n",
       "\n",
       "[10289 rows x 11 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = mlf.preprocess(df)\n",
    "prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.5 Fit the Model**\n",
    "\n",
    "The fit method uses the following parameters:\n",
    "|Parameters |Type\t|Default\t|Details|\n",
    "|-----------|-------|-----------|-------|\n",
    "|df|\tDataFrame|\t\t|Series data in long format.|\n",
    "|id_col|\tstr\t|unique_id|\tColumn that identifies each serie|.\n",
    "|time_col\t|str\t|ds\t|Column that identifies each timestep, its values can be timestamps or integers.\n",
    "|target_col\t|str\t|y\t|Column that contains the target.\n",
    "|static_features\t|typing.Optional[typing.List[str]]|\tNone\t|Names of the features that are static and will be repeated when forecasting. If None, will consider all columns (except id_col and time_col) as static.|\n",
    "|dropna\t|bool\t|True\t|Drop rows with missing values produced by the transformations.|\n",
    "|keep_last_n\t|typing.Optional[int]\t|None\t|Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.|\n",
    "|max_horizon\t|typing.Optional[int]\t|None\t|\n",
    "|prediction_intervals\t|typing.Optional[mlforecast.utils.PredictionIntervals]\t|None\t|Configuration to calibrate prediction intervals (Conformal Prediction).|\n",
    "|fitted\t|bool\t|False\t|Save in-sample predictions.|\n",
    "|data\t|typing.Optional[pandas.core.frame.DataFrame]\t|None\t|Series data in long format. This argument has been replaced by df and will be removed in a later release.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[XGBRegressor], freq=<30 * Minutes>, lag_features=['lag1', 'lag24', 'expanding_mean_lag1', 'rolling_mean_lag24_window_size7'], date_features=['year', 'month', 'day', 'hour'], num_threads=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the models\n",
    "mlf.fit(df,  \n",
    " fitted=True, \n",
    "prediction_intervals=PredictionIntervals(n_windows=5, window_size=30, method=\"conformal_distribution\" )  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results of our model in this case the `XGBoost model`. We can observe it with the following instruction:\n",
    "\n",
    "Let us now visualize the fitted values of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>XGBRegressor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 15:30:00</td>\n",
       "      <td>18544</td>\n",
       "      <td>-1136.708496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 16:00:00</td>\n",
       "      <td>16228</td>\n",
       "      <td>-2054.171143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 16:30:00</td>\n",
       "      <td>15013</td>\n",
       "      <td>-1122.270996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 17:00:00</td>\n",
       "      <td>17203</td>\n",
       "      <td>2349.349121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 17:30:00</td>\n",
       "      <td>19525</td>\n",
       "      <td>2475.052734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-31 21:30:00</td>\n",
       "      <td>24670</td>\n",
       "      <td>1082.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-31 22:00:00</td>\n",
       "      <td>25721</td>\n",
       "      <td>1142.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-31 22:30:00</td>\n",
       "      <td>27309</td>\n",
       "      <td>1471.630493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-31 23:00:00</td>\n",
       "      <td>26591</td>\n",
       "      <td>-242.068710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-31 23:30:00</td>\n",
       "      <td>26288</td>\n",
       "      <td>-645.658569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10289 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ds      y  XGBRegressor\n",
       "unique_id                                         \n",
       "1         2014-07-01 15:30:00  18544  -1136.708496\n",
       "1         2014-07-01 16:00:00  16228  -2054.171143\n",
       "1         2014-07-01 16:30:00  15013  -1122.270996\n",
       "1         2014-07-01 17:00:00  17203   2349.349121\n",
       "1         2014-07-01 17:30:00  19525   2475.052734\n",
       "...                       ...    ...           ...\n",
       "1         2015-01-31 21:30:00  24670   1082.906250\n",
       "1         2015-01-31 22:00:00  25721   1142.089600\n",
       "1         2015-01-31 22:30:00  27309   1471.630493\n",
       "1         2015-01-31 23:00:00  26591   -242.068710\n",
       "1         2015-01-31 23:30:00  26288   -645.658569\n",
       "\n",
       "[10289 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=mlf.forecast_fitted_values()\n",
    "result=result.set_index(\"unique_id\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import normal_ad\n",
    "from scipy import stats\n",
    "\n",
    "sw_result = stats.shapiro(result[\"XGBRegressor\"])\n",
    "ad_result = normal_ad(np.array(result[\"XGBRegressor\"]), axis=0)\n",
    "dag_result = stats.normaltest(result[\"XGBRegressor\"], axis=0, nan_policy='propagate')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that we can only use this method if we assume that the residuals of our validation predictions are normally distributed. To see if this is the case, we will use a PP-plot and test its normality with the Anderson-Darling, Kolmogorov-Smirnov, and D’Agostino K^2 tests.\n",
    "\n",
    "The PP-plot(Probability-to-Probability) plots the data sample against the normal distribution plot in such a way that if normally distributed, the data points will form a straight line.\n",
    "\n",
    "The three normality tests determine how likely a data sample is from a normally distributed population using p-values. The null hypothesis for each test is that \"the sample came from a normally distributed population\". This means that if the resulting p-values are below a chosen alpha value, then the null hypothesis is rejected. Thus there is evidence to suggest that the data comes from a non-normal distribution. For this article, we will use an Alpha value of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=mlf.forecast_fitted_values()\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "# plot[1,1]\n",
    "result[\"XGBRegressor\"].plot(ax=axs[0,0])\n",
    "axs[0,0].set_title(\"Residuals model\");\n",
    "\n",
    "# plot\n",
    "#plot(result[\"XGBRegressor\"], ax=axs[0,1]);\n",
    "axs[0,1].hist(result[\"XGBRegressor\"], density=True,bins=50, alpha=0.5 )\n",
    "axs[0,1].set_title(\"Density plot - Residual\");\n",
    "\n",
    "# plot\n",
    "stats.probplot(result[\"XGBRegressor\"], dist=\"norm\", plot=axs[1,0])\n",
    "axs[1,0].set_title('Plot Q-Q')\n",
    "axs[1,0].annotate(\"SW p-val: {:.4f}\".format(sw_result[1]), xy=(0.05,0.9), xycoords='axes fraction', fontsize=15,\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\", pad=0.6))\n",
    "\n",
    "axs[1,0].annotate(\"AD p-val: {:.4f}\".format(ad_result[1]), xy=(0.05,0.8), xycoords='axes fraction', fontsize=15,\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\", pad=0.6))\n",
    "\n",
    "axs[1,0].annotate(\"DAG p-val: {:.4f}\".format(dag_result[1]), xy=(0.05,0.7), xycoords='axes fraction', fontsize=15,\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\", pad=0.6))\n",
    "# plot\n",
    "plot_acf(result[\"XGBRegressor\"],  lags=35, ax=axs[1,1],color=\"fuchsia\")\n",
    "axs[1,1].set_title(\"Autocorrelation\");\n",
    "\n",
    "plt.savefig(\"../figs/prediction_intervals_in_forecasting_models__plot_residual_model.png\")\n",
    "plt.close();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/prediction_intervals_in_forecasting_models__plot_residual_model.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.6 Predict method with prediction intervals**\n",
    "\n",
    "To generate forecasts use the predict method.\n",
    "\n",
    "The predict method takes several arguments.: \n",
    "\n",
    "|Parameters |Type\t|Default\t|Details|\n",
    "|-----------|-------|-----------|-------|\n",
    "|h\t|int\t|\t|Number of periods to predict.|\n",
    "|dynamic_dfs\t|typing.Optional[typing.List[pandas.core.frame.DataFrame]]\t|None\t|Future values of the dynamic features, e.g. prices.\n",
    "|before_predict_callback\t|typing.Optional[typing.Callable]\t|None\t|Function to call on the features before computing the predictions. This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure. The series identifier is on the index.\n",
    "|after_predict_callback\t|typing.Optional[typing.Callable]\t|None\t|Function to call on the predictions before updating the targets. This function will take a pandas Series with the predictions and should return another one with the same structure. The series identifier is on the index.\n",
    "|new_df\t|typing.Optional[pandas.core.frame.DataFrame]|\tNone\t|Series data of new observations for which forecasts are to be generated. This dataframe should have the same structure as the one used to fit the model, including any features and time series data. If new_df is not None, the method will generate forecasts for the new observations.\n",
    "|level\t|typing.Optional[typing.List[typing.Union[int, float]]]|\tNone\t|Confidence levels between 0 and 100 for prediction intervals.\n",
    "|X_df\t|typing.Optional[pandas.core.frame.DataFrame]\t|None\t|Dataframe with the future exogenous features. Should have the id column and the time column.\n",
    "|ids\t|typing.Optional[typing.List[str]]\t|None\t|List with subset of ids seen during training for which the forecasts should be computed.\n",
    "|horizon\t|typing.Optional[int]|\tNone\t|Number of periods to predict. This argument has been replaced by h and will be removed in a later release.\n",
    "|new_data\t|typing.Optional[pandas.core.frame.DataFrame]\t|None\t|Series data of new observations for which forecasts are to be generated. This dataframe should have the same structure as the one used to fit the model, including any features and time series data. If new_data is not None, the method will generate forecasts for the new observation\n",
    "\n",
    "\n",
    "\n",
    "The forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n",
    "\n",
    "This step should take less than 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>XGBRegressor-lo-95</th>\n",
       "      <th>XGBRegressor-lo-80</th>\n",
       "      <th>XGBRegressor-hi-80</th>\n",
       "      <th>XGBRegressor-hi-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 00:00:00</td>\n",
       "      <td>24608.865234</td>\n",
       "      <td>24016.475873</td>\n",
       "      <td>24085.588062</td>\n",
       "      <td>25132.142407</td>\n",
       "      <td>25201.254596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 00:30:00</td>\n",
       "      <td>23323.097656</td>\n",
       "      <td>20511.105615</td>\n",
       "      <td>21901.008398</td>\n",
       "      <td>24745.186914</td>\n",
       "      <td>26135.089697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 01:00:00</td>\n",
       "      <td>22223.435547</td>\n",
       "      <td>20161.902002</td>\n",
       "      <td>20995.971289</td>\n",
       "      <td>23450.899805</td>\n",
       "      <td>24284.969092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 01:30:00</td>\n",
       "      <td>20405.228516</td>\n",
       "      <td>17227.147949</td>\n",
       "      <td>17822.294922</td>\n",
       "      <td>22988.162109</td>\n",
       "      <td>23583.309082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 02:00:00</td>\n",
       "      <td>20014.324219</td>\n",
       "      <td>17422.155518</td>\n",
       "      <td>17923.692383</td>\n",
       "      <td>22104.956055</td>\n",
       "      <td>22606.492920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 02:30:00</td>\n",
       "      <td>18003.347656</td>\n",
       "      <td>14115.529980</td>\n",
       "      <td>15098.559375</td>\n",
       "      <td>20908.135937</td>\n",
       "      <td>21891.165332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 03:00:00</td>\n",
       "      <td>15388.673828</td>\n",
       "      <td>11592.610205</td>\n",
       "      <td>13597.456445</td>\n",
       "      <td>17179.891211</td>\n",
       "      <td>19184.737451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 03:30:00</td>\n",
       "      <td>13329.220703</td>\n",
       "      <td>8739.386230</td>\n",
       "      <td>10764.214844</td>\n",
       "      <td>15894.226562</td>\n",
       "      <td>17919.055176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 04:00:00</td>\n",
       "      <td>10769.670898</td>\n",
       "      <td>6428.871289</td>\n",
       "      <td>7464.490039</td>\n",
       "      <td>14074.851758</td>\n",
       "      <td>15110.470508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 04:30:00</td>\n",
       "      <td>7761.180176</td>\n",
       "      <td>3116.526367</td>\n",
       "      <td>3461.264160</td>\n",
       "      <td>12061.096191</td>\n",
       "      <td>12405.833984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 05:00:00</td>\n",
       "      <td>5767.234375</td>\n",
       "      <td>827.160938</td>\n",
       "      <td>2496.762891</td>\n",
       "      <td>9037.705859</td>\n",
       "      <td>10707.307813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 05:30:00</td>\n",
       "      <td>4922.226562</td>\n",
       "      <td>-360.123730</td>\n",
       "      <td>1953.958203</td>\n",
       "      <td>7890.494922</td>\n",
       "      <td>10204.576855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 06:00:00</td>\n",
       "      <td>4861.806152</td>\n",
       "      <td>-2511.661230</td>\n",
       "      <td>1182.186621</td>\n",
       "      <td>8541.425684</td>\n",
       "      <td>12235.273535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 06:30:00</td>\n",
       "      <td>5627.834473</td>\n",
       "      <td>-2723.522559</td>\n",
       "      <td>435.783301</td>\n",
       "      <td>10819.885645</td>\n",
       "      <td>13979.191504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 07:00:00</td>\n",
       "      <td>6474.878418</td>\n",
       "      <td>-2472.229053</td>\n",
       "      <td>2188.001270</td>\n",
       "      <td>10761.755566</td>\n",
       "      <td>15421.985889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 07:30:00</td>\n",
       "      <td>7665.793457</td>\n",
       "      <td>-924.076709</td>\n",
       "      <td>4742.826465</td>\n",
       "      <td>10588.760449</td>\n",
       "      <td>16255.663623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 08:00:00</td>\n",
       "      <td>9841.162109</td>\n",
       "      <td>1587.783215</td>\n",
       "      <td>7069.405811</td>\n",
       "      <td>12612.918408</td>\n",
       "      <td>18094.541003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 08:30:00</td>\n",
       "      <td>11555.568359</td>\n",
       "      <td>3651.682397</td>\n",
       "      <td>7769.709082</td>\n",
       "      <td>15341.427637</td>\n",
       "      <td>19459.454321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 09:00:00</td>\n",
       "      <td>14289.201172</td>\n",
       "      <td>6586.853894</td>\n",
       "      <td>8775.857471</td>\n",
       "      <td>19802.544873</td>\n",
       "      <td>21991.548450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 09:30:00</td>\n",
       "      <td>15288.630859</td>\n",
       "      <td>7847.298401</td>\n",
       "      <td>8886.259521</td>\n",
       "      <td>21691.002197</td>\n",
       "      <td>22729.963318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 10:00:00</td>\n",
       "      <td>17081.392578</td>\n",
       "      <td>8856.845898</td>\n",
       "      <td>9753.430469</td>\n",
       "      <td>24409.354687</td>\n",
       "      <td>25305.939258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 10:30:00</td>\n",
       "      <td>19002.509766</td>\n",
       "      <td>11193.393076</td>\n",
       "      <td>14548.455115</td>\n",
       "      <td>23456.564417</td>\n",
       "      <td>26811.626456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 11:00:00</td>\n",
       "      <td>20050.558594</td>\n",
       "      <td>12926.887451</td>\n",
       "      <td>15278.333008</td>\n",
       "      <td>24822.784180</td>\n",
       "      <td>27174.229736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 11:30:00</td>\n",
       "      <td>21006.652344</td>\n",
       "      <td>14009.135718</td>\n",
       "      <td>16063.265527</td>\n",
       "      <td>25950.039160</td>\n",
       "      <td>28004.168970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 12:00:00</td>\n",
       "      <td>21353.490234</td>\n",
       "      <td>13244.908716</td>\n",
       "      <td>16511.532324</td>\n",
       "      <td>26195.448145</td>\n",
       "      <td>29462.071753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 12:30:00</td>\n",
       "      <td>21630.076172</td>\n",
       "      <td>11808.785815</td>\n",
       "      <td>17407.438184</td>\n",
       "      <td>25852.714160</td>\n",
       "      <td>31451.366528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 13:00:00</td>\n",
       "      <td>21595.931641</td>\n",
       "      <td>9135.145190</td>\n",
       "      <td>17520.229199</td>\n",
       "      <td>25671.634082</td>\n",
       "      <td>34056.718091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 13:30:00</td>\n",
       "      <td>21700.009766</td>\n",
       "      <td>6524.492212</td>\n",
       "      <td>17685.702246</td>\n",
       "      <td>25714.317285</td>\n",
       "      <td>36875.527319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 14:00:00</td>\n",
       "      <td>22105.839844</td>\n",
       "      <td>5004.823792</td>\n",
       "      <td>16505.282959</td>\n",
       "      <td>27706.396729</td>\n",
       "      <td>39206.855896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-01 14:30:00</td>\n",
       "      <td>21736.753906</td>\n",
       "      <td>3294.232184</td>\n",
       "      <td>17638.412134</td>\n",
       "      <td>25835.095679</td>\n",
       "      <td>40179.275629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id                  ds  XGBRegressor  XGBRegressor-lo-95  \\\n",
       "0          1 2015-02-01 00:00:00  24608.865234        24016.475873   \n",
       "1          1 2015-02-01 00:30:00  23323.097656        20511.105615   \n",
       "2          1 2015-02-01 01:00:00  22223.435547        20161.902002   \n",
       "3          1 2015-02-01 01:30:00  20405.228516        17227.147949   \n",
       "4          1 2015-02-01 02:00:00  20014.324219        17422.155518   \n",
       "5          1 2015-02-01 02:30:00  18003.347656        14115.529980   \n",
       "6          1 2015-02-01 03:00:00  15388.673828        11592.610205   \n",
       "7          1 2015-02-01 03:30:00  13329.220703         8739.386230   \n",
       "8          1 2015-02-01 04:00:00  10769.670898         6428.871289   \n",
       "9          1 2015-02-01 04:30:00   7761.180176         3116.526367   \n",
       "10         1 2015-02-01 05:00:00   5767.234375          827.160938   \n",
       "11         1 2015-02-01 05:30:00   4922.226562         -360.123730   \n",
       "12         1 2015-02-01 06:00:00   4861.806152        -2511.661230   \n",
       "13         1 2015-02-01 06:30:00   5627.834473        -2723.522559   \n",
       "14         1 2015-02-01 07:00:00   6474.878418        -2472.229053   \n",
       "15         1 2015-02-01 07:30:00   7665.793457         -924.076709   \n",
       "16         1 2015-02-01 08:00:00   9841.162109         1587.783215   \n",
       "17         1 2015-02-01 08:30:00  11555.568359         3651.682397   \n",
       "18         1 2015-02-01 09:00:00  14289.201172         6586.853894   \n",
       "19         1 2015-02-01 09:30:00  15288.630859         7847.298401   \n",
       "20         1 2015-02-01 10:00:00  17081.392578         8856.845898   \n",
       "21         1 2015-02-01 10:30:00  19002.509766        11193.393076   \n",
       "22         1 2015-02-01 11:00:00  20050.558594        12926.887451   \n",
       "23         1 2015-02-01 11:30:00  21006.652344        14009.135718   \n",
       "24         1 2015-02-01 12:00:00  21353.490234        13244.908716   \n",
       "25         1 2015-02-01 12:30:00  21630.076172        11808.785815   \n",
       "26         1 2015-02-01 13:00:00  21595.931641         9135.145190   \n",
       "27         1 2015-02-01 13:30:00  21700.009766         6524.492212   \n",
       "28         1 2015-02-01 14:00:00  22105.839844         5004.823792   \n",
       "29         1 2015-02-01 14:30:00  21736.753906         3294.232184   \n",
       "\n",
       "    XGBRegressor-lo-80  XGBRegressor-hi-80  XGBRegressor-hi-95  \n",
       "0         24085.588062        25132.142407        25201.254596  \n",
       "1         21901.008398        24745.186914        26135.089697  \n",
       "2         20995.971289        23450.899805        24284.969092  \n",
       "3         17822.294922        22988.162109        23583.309082  \n",
       "4         17923.692383        22104.956055        22606.492920  \n",
       "5         15098.559375        20908.135937        21891.165332  \n",
       "6         13597.456445        17179.891211        19184.737451  \n",
       "7         10764.214844        15894.226562        17919.055176  \n",
       "8          7464.490039        14074.851758        15110.470508  \n",
       "9          3461.264160        12061.096191        12405.833984  \n",
       "10         2496.762891         9037.705859        10707.307813  \n",
       "11         1953.958203         7890.494922        10204.576855  \n",
       "12         1182.186621         8541.425684        12235.273535  \n",
       "13          435.783301        10819.885645        13979.191504  \n",
       "14         2188.001270        10761.755566        15421.985889  \n",
       "15         4742.826465        10588.760449        16255.663623  \n",
       "16         7069.405811        12612.918408        18094.541003  \n",
       "17         7769.709082        15341.427637        19459.454321  \n",
       "18         8775.857471        19802.544873        21991.548450  \n",
       "19         8886.259521        21691.002197        22729.963318  \n",
       "20         9753.430469        24409.354687        25305.939258  \n",
       "21        14548.455115        23456.564417        26811.626456  \n",
       "22        15278.333008        24822.784180        27174.229736  \n",
       "23        16063.265527        25950.039160        28004.168970  \n",
       "24        16511.532324        26195.448145        29462.071753  \n",
       "25        17407.438184        25852.714160        31451.366528  \n",
       "26        17520.229199        25671.634082        34056.718091  \n",
       "27        17685.702246        25714.317285        36875.527319  \n",
       "28        16505.282959        27706.396729        39206.855896  \n",
       "29        17638.412134        25835.095679        40179.275629  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_df = mlf.predict(h=30, level=[80,95]) \n",
    "\n",
    "forecast_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.7 Plot prediction intervals**\n",
    "\n",
    "Now let's visualize the result of our forecast and the historical data of our time series, also let's draw the confidence interval that we have obtained when making the prediction with 95% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plot_series(df, forecast_df, level=[80,95], max_insample_length=200,engine=\"matplotlib\")\n",
    "for ax in fig.get_axes():\n",
    "   ax.set_title(\"Prediction intervals\")\n",
    "fig.savefig('../figs/prediction_intervals_in_forecasting_models__plot_forecasting_intervals.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/prediction_intervals_in_forecasting_models__plot_forecasting_intervals.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/prediction_intervals_in_forecasting_models__plot_forecasting_interval.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence interval is a range of values that has a high probability of containing the true value of a variable. In machine learning time series models, the confidence interval is used to estimate the uncertainty in the predictions.\n",
    "\n",
    "One of the main benefits of using the confidence interval is that it allows users to understand the accuracy of the predictions. For example, if the confidence interval is very wide, it means that the prediction is less accurate. Conversely, if the confidence interval is very narrow, it means that the prediction is more accurate.\n",
    "\n",
    "Another benefit of the confidence interval is that it helps users make informed decisions. For example, if a prediction is within the confidence interval, it means that it is likely to come true. Conversely, if a prediction is outside the confidence interval, it means that it is less likely to come true.\n",
    "\n",
    "In general, the confidence interval is an important tool for machine learning time series models. It helps users understand the accuracy of the forecasts and make informed decisions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. References** <a class=\"anchor\" id=\"10\"></a>\n",
    "\n",
    "[Table of Contents](#0)\n",
    "\n",
    "1. Changquan Huang • Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python. \n",
    "2. Ivan Svetunkov. [Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)](https://openforecast.org/adam/)\n",
    "3. [James D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.](https://press.princeton.edu/books/hardcover/9780691042893/time-series-analysis)\n",
    "4. [Nixtla Parameters for Mlforecast](https://nixtla.github.io/mlforecast/forecast.html).\n",
    "5. [Pandas available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "6. [Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Time series cross-validation”.](https://otexts.com/fpp3/tscv.html).\n",
    "7. [Seasonal periods- Rob J Hyndman](https://robjhyndman.com/hyndsight/seasonal-periods/).\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statsforecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
