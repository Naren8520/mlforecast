{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of the following article is to obtain a step-by-step guide on how to make a `Pipeline` with `Sklearn` for our Time Series model using `Mlforecast`.\n",
    "\n",
    "During this tutorial, we will get familiar with the main class `MlForecast` and some relevant methods such as `mlforecast.fit`, `mlforecast.predict` and `mlforecast.cross_validation` among others.\n",
    "\n",
    "Let us begin!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "\n",
    "\n",
    "\n",
    "1.\t[Introduction](#1)\n",
    "2.\t[Loading libraries and Data](#3)\n",
    "3.\t[Explore Data with the plot method](#3)\n",
    "4.\t[Training A Multivariate Time Series Model With MLForecast](#4)\n",
    "5.  [Feature importances](#5)\n",
    "6.  [Evaluate the model’s performance](#6)\n",
    "7.  [Evaluate the model](#7)\n",
    "8.  [References](#8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Table of Contents](#0)\n",
    "\n",
    "A pipeline in the context of machine learning refers to a sequence of steps organized in a structured way to process and transform data before training a model and making predictions. The main goal of a pipeline is to automate and standardize the workflow, which facilitates reproducibility and the deployment of models in production.\n",
    "\n",
    "A typical machine learning pipeline consists of the following stages:\n",
    "\n",
    "1. **Data preprocessing**: In this stage, various tasks are performed to prepare the data before training the model. This may include data cleaning, handling of missing values, normalization or standardization of variables, coding of categorical variables, selection of relevant features, among others.\n",
    "\n",
    "2. **Data Splitting**: It is common to split data into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters and evaluate performance during model tuning, and the test set is used to evaluate the final performance of the model.\n",
    "\n",
    "3. **Model training**: In this stage, the machine learning model is selected and trained using the training data. This involves tuning the model parameters based on the training data to minimize the loss function or maximize the target performance metric.\n",
    "\n",
    "4. **Model validation**: After training the model, its performance is evaluated using the validation data. This allows tuning the model's hyperparameters and making comparisons between different configurations or algorithms.\n",
    "\n",
    "5. **Model evaluation**: Once the hyperparameters have been tuned and the best model has been selected, its final performance is evaluated using the test set. This provides an unbiased estimate of model performance on unseen data.\n",
    "\n",
    "6. **Predictions**: Finally, the trained model is used to make predictions on new data or real-time data.\n",
    "\n",
    "Using a pipeline in machine learning has several benefits, such as automating repetitive tasks, standardizing workflow, ease of reproducing results, and the ability to scale and apply the model in production efficiently.\n",
    "\n",
    "It is important to note that the specific steps and order of a pipeline can vary depending on the problem and project requirements. Additionally, additional stages can be included, such as selecting the best model or optimizing hyperparameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading libraries and Data** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling and processing of Data\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Handling and processing of Data for Date (time)\n",
    "# ==============================================================================\n",
    "import datetime\n",
    "import datetime as dt\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# \n",
    "# ==============================================================================\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models Sklearn\n",
    "# ==============================================================================\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Mlforecast\n",
    "# ==============================================================================\n",
    "from mlforecast import MLForecast\n",
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "from window_ops.ewm import ewm_mean\n",
    "from mlforecast.target_transforms import Differences\n",
    "\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series\n",
    "from utilsforecast.plotting import plot_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plt.style.use('grayscale') # fivethirtyeight  grayscale  classic\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "\n",
    "# Define the plot size\n",
    "# ==============================================================================\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18,7)\n",
    "\n",
    "# Hide warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read data**\n",
    "\n",
    "We will use data from the [City of Los Angeles website traffic dataset](https://www.kaggle.com/datasets/cityofLA/lacity.org-website-traffic) on Kaggle.\n",
    "\n",
    "This dataset contains the number of user sessions for the City of Los Angeles website for each day from 2014 to 2019.\n",
    "\n",
    "Sessions are periods of time when a user is active in a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Device Category</th>\n",
       "      <th>Browser</th>\n",
       "      <th># of Visitors</th>\n",
       "      <th>Sessions</th>\n",
       "      <th>Bounce Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01T00:00:00.000</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>900</td>\n",
       "      <td>934</td>\n",
       "      <td>55.5675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01T00:00:00.000</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>692</td>\n",
       "      <td>761</td>\n",
       "      <td>40.8673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01T00:00:00.000</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Internet Explorer</td>\n",
       "      <td>1038</td>\n",
       "      <td>1107</td>\n",
       "      <td>31.2556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01T00:00:00.000</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Opera</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01T00:00:00.000</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Safari</td>\n",
       "      <td>484</td>\n",
       "      <td>554</td>\n",
       "      <td>24.9097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date Device Category            Browser  # of Visitors  \\\n",
       "0  2014-01-01T00:00:00.000         desktop             Chrome            900   \n",
       "1  2014-01-01T00:00:00.000         desktop            Firefox            692   \n",
       "2  2014-01-01T00:00:00.000         desktop  Internet Explorer           1038   \n",
       "3  2014-01-01T00:00:00.000         desktop              Opera             35   \n",
       "4  2014-01-01T00:00:00.000         desktop             Safari            484   \n",
       "\n",
       "   Sessions  Bounce Rate  \n",
       "0       934      55.5675  \n",
       "1       761      40.8673  \n",
       "2      1107      31.2556  \n",
       "3        35     100.0000  \n",
       "4       554      24.9097  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"lacity.org-website-traffic.csv\", ) #.loc[:, ['Date', 'Device Category', 'Sessions']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>device_category</th>\n",
       "      <th>browser</th>\n",
       "      <th>qnty_of_visitors</th>\n",
       "      <th>sessions</th>\n",
       "      <th>bounce_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>900</td>\n",
       "      <td>934</td>\n",
       "      <td>55.5675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>692</td>\n",
       "      <td>761</td>\n",
       "      <td>40.8673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Internet Explorer</td>\n",
       "      <td>1038</td>\n",
       "      <td>1107</td>\n",
       "      <td>31.2556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Opera</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Safari</td>\n",
       "      <td>484</td>\n",
       "      <td>554</td>\n",
       "      <td>24.9097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348980</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>199</td>\n",
       "      <td>318</td>\n",
       "      <td>50.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348981</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348982</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Safari</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>79.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348983</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>tablet</td>\n",
       "      <td>Amazon Silk</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348984</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>tablet</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8348985 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date device_category            browser  qnty_of_visitors  \\\n",
       "0       2014-01-01         desktop             Chrome               900   \n",
       "1       2014-01-01         desktop            Firefox               692   \n",
       "2       2014-01-01         desktop  Internet Explorer              1038   \n",
       "3       2014-01-01         desktop              Opera                35   \n",
       "4       2014-01-01         desktop             Safari               484   \n",
       "...            ...             ...                ...               ...   \n",
       "8348980 2019-08-27          mobile             Chrome               199   \n",
       "8348981 2019-08-27          mobile            Firefox                40   \n",
       "8348982 2019-08-27          mobile             Safari               199   \n",
       "8348983 2019-08-27          tablet        Amazon Silk                40   \n",
       "8348984 2019-08-27          tablet             Chrome                40   \n",
       "\n",
       "         sessions  bounce_rate  \n",
       "0             934      55.5675  \n",
       "1             761      40.8673  \n",
       "2            1107      31.2556  \n",
       "3              35     100.0000  \n",
       "4             554      24.9097  \n",
       "...           ...          ...  \n",
       "8348980       318      50.0000  \n",
       "8348981        40     100.0000  \n",
       "8348982       199      79.8995  \n",
       "8348983        40     100.0000  \n",
       "8348984        40     100.0000  \n",
       "\n",
       "[8348985 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [k.lower().replace(' ', '_').replace('#', 'qnty') for k in df.columns]\n",
    "df['date'] = pd.to_datetime(df.date).dt.normalize()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data based on Date, Device and Session.\n",
    "data = df[['date', 'device_category', 'sessions']].groupby(['date', 'device_category']).sum().reset_index()\n",
    "\n",
    "# Fill Missing Any Date to 0\n",
    "df_pivot = data.pivot(index='device_category', values=['sessions'], columns='date')\n",
    "df_pivot.fillna(0, inplace=True)\n",
    "\n",
    "df_pivot = df_pivot.stack().reset_index().sort_values(by=['date', 'device_category']).reset_index(drop=True)\n",
    "df_pivot.columns = ['device', 'date', 'session']\n",
    "data = df_pivot[['date', 'device', 'session']]\n",
    "\n",
    "del df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6195 entries, 0 to 6194\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   date     6195 non-null   datetime64[ns]\n",
      " 1   device   6195 non-null   object        \n",
      " 2   session  6195 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 145.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to MlForecast is always a data frame in long format with three columns: unique_id, ds and y:\n",
    "\n",
    "* The `unique_id` (string, int or category) represents an identifier for the series.\n",
    "\n",
    "* The `ds` (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n",
    "\n",
    "* The `y` (numeric) represents the measurement we wish to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>desktop</td>\n",
       "      <td>616768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>mobile</td>\n",
       "      <td>287547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>tablet</td>\n",
       "      <td>15967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2030513.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>mobile</td>\n",
       "      <td>321557.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ds unique_id          y\n",
       "0 2014-01-01   desktop   616768.0\n",
       "1 2014-01-01    mobile   287547.0\n",
       "2 2014-01-01    tablet    15967.0\n",
       "3 2014-01-02   desktop  2030513.0\n",
       "4 2014-01-02    mobile   321557.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.rename(columns={\"date\": \"ds\",\"device\":\"unique_id\", \"session\": \"y\"})\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Explore Data with the plot method** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Table of Contents](#0)\n",
    "\n",
    "We are going to use the `plot_series` function to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plot_series(data, palette=\"plasma\", engine=\"matplotlib\")\n",
    "fig.savefig('../figs/pipeline_with_sklearn_and_xgboost_random_forest__eda.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/pipeline_with_sklearn_and_xgboost_random_forest__eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Augmented Dickey-Fuller Test**\n",
    "An Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\n",
    "\n",
    "Augmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\n",
    "\n",
    "- Null Hypothesis: Time Series is non-stationary. It gives a time-dependent trend.\n",
    "- Alternate Hypothesis: Time Series is stationary. In another term, the series doesn’t depend on time.\n",
    "\n",
    "- ADF or t Statistic < critical values: Reject the null hypothesis, time series is stationary.\n",
    "- ADF or t Statistic > critical values: Failed to reject the null hypothesis, time series is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_dickey_fuller_test(series , column_name):\n",
    "    print (f'Dickey-Fuller test results for columns: {column_name}')\n",
    "    dftest = adfuller(series, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n",
    "    for key,value in dftest[4].items():\n",
    "       dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "    if dftest[1] <= 0.05:\n",
    "        print(\"Conclusion:====>\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"The data is stationary\")\n",
    "    else:\n",
    "        print(\"Conclusion:====>\")\n",
    "        print(\"The null hypothesis cannot be rejected\")\n",
    "        print(\"The data is not stationary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dickey-Fuller test results for columns: website-traffic\n",
      "Test Statistic                -7.269285e+00\n",
      "p-value                        1.604152e-10\n",
      "No Lags Used                   3.400000e+01\n",
      "Number of observations used    6.160000e+03\n",
      "Critical Value (1%)           -3.431412e+00\n",
      "Critical Value (5%)           -2.862009e+00\n",
      "Critical Value (10%)          -2.567020e+00\n",
      "dtype: float64\n",
      "Conclusion:====>\n",
      "Reject the null hypothesis\n",
      "The data is stationary\n"
     ]
    }
   ],
   "source": [
    "augmented_dickey_fuller_test(data[\"y\"],\"website-traffic\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training A Multivariate Time Series Model With MLForecast**<a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building Model**\n",
    "\n",
    "Let’s see how we can engineer features and train an `XGBoost` and `RandomForest` with mlforecast.\n",
    "\n",
    "In this case we are going to create a `Pipeline` with the `make_pileline` function in which we are going to use the `SimpleImputer` in case we have null data, and we will also use the `StandardScaler` for the standardization of the data. Let's remember that the `StandarScaler` and the `SimpleImputer` are executed in the fit method.\n",
    "\n",
    "We have added the `StandardScaler` function to the construction of the model that we are going to use, however we can create this same function and use it in the `differences` parameter, however for the sake of exemplifying this tutorial we are going to use the `StandardScaler` from the construction of the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "models = [make_pipeline(SimpleImputer(), \n",
    "                        StandardScaler(),\n",
    "                        PowerTransformer(method='yeo-johnson'),\n",
    "                        RandomForestRegressor(random_state=0, n_estimators=100)),\n",
    "                        XGBRegressor(random_state=0, n_estimators=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(steps=[('simpleimputer', SimpleImputer()),\n",
       "                 ('standardscaler', StandardScaler()),\n",
       "                 ('powertransformer', PowerTransformer()),\n",
       "                 ('randomforestregressor',\n",
       "                  RandomForestRegressor(random_state=0))]),\n",
       " XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, ...)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the models by instantiating a new `MlForecast` object with the following parameters:\n",
    "\n",
    "* `models:` a list of models. Select the models you want from models and import them.\n",
    "\n",
    "* `freq:` a string indicating the frequency of the data. (See [panda’s available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).)\n",
    "\n",
    "* `lags:` Lags of the target to uses as feature.\n",
    "\n",
    "* `lag_transforms:` Mapping of target lags to their transformations.\n",
    "\n",
    "* `date_features:` Features computed from the dates. Can be `pandas` date attributes or functions that will take the dates as input.\n",
    "\n",
    "* `differences:` Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "\n",
    "* `num_threads:` Number of threads to use when computing the features.\n",
    "\n",
    "* `target_transforms:` Transformations that will be applied to the target computing the features and restored after the forecasting step.\n",
    "\n",
    "Any settings are passed into the constructor. Then you call its fit method and pass in the historical data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean, rolling_max, rolling_min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lags` argument is a list of lags we want to use in our model. Lags are the number of steps in the past we want to use to predict the future.\n",
    "\n",
    "So in our case, we are using the value of the target variable 1, 7, and 14 days before the timestamp date of the observation.\n",
    "\n",
    "This is why it’s important to have all dates, even the ones with missing values and zeros in the datasets, so these features can be computed correctly.\n",
    "\n",
    "We also pass the `lag_transforms` argument, which is a dictionary with the `lag` as the key and a list of functions as the value.\n",
    "\n",
    "These functions will be applied to the lagged series. For example, we are applying a `rolling mean` with a window of 7 days to the lag 1.\n",
    "\n",
    "This means that we will have a feature that is the mean of the 7 last values of the target variable after shifting it 1 day.\n",
    "\n",
    "We are using the `window_ops` library which is optimized and recommended for MLForecast.\n",
    "\n",
    "Each tuple in the list of transforms is a function and its arguments. For example, the `rolling_mean` function takes a window size as its argument.\n",
    "\n",
    "The best value for the window size is specific for each time series, so we need to try different values and see which one works best.\n",
    "\n",
    "The `date_features` argument is a list of date components we want to extract from the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MLForecast(models=models,\n",
    "                   freq='D',\n",
    "                   lags=[1,7,14],\n",
    "                   lag_transforms={\n",
    "                       1: [(rolling_mean, 4), (rolling_min, 4), (rolling_max, 4)], 1: [expanding_mean]},\n",
    "                   date_features=['week', 'month','day'],\n",
    "                   num_threads=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `preprocess` function we can observe all the transformation that has been made of the target variable before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>y</th>\n",
       "      <th>lag1</th>\n",
       "      <th>lag7</th>\n",
       "      <th>lag14</th>\n",
       "      <th>expanding_mean_lag1</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2489993.0</td>\n",
       "      <td>2595781.0</td>\n",
       "      <td>3213820.0</td>\n",
       "      <td>616768.0</td>\n",
       "      <td>2.018989e+06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>mobile</td>\n",
       "      <td>158945.0</td>\n",
       "      <td>391598.0</td>\n",
       "      <td>1022962.0</td>\n",
       "      <td>287547.0</td>\n",
       "      <td>4.074675e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>tablet</td>\n",
       "      <td>61490.0</td>\n",
       "      <td>88987.0</td>\n",
       "      <td>214816.0</td>\n",
       "      <td>15967.0</td>\n",
       "      <td>1.038917e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2134761.0</td>\n",
       "      <td>2489993.0</td>\n",
       "      <td>3522842.0</td>\n",
       "      <td>2030513.0</td>\n",
       "      <td>2.050389e+06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>mobile</td>\n",
       "      <td>339028.0</td>\n",
       "      <td>158945.0</td>\n",
       "      <td>691006.0</td>\n",
       "      <td>321557.0</td>\n",
       "      <td>3.908993e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds unique_id          y       lag1       lag7      lag14  \\\n",
       "42 2014-01-15   desktop  2489993.0  2595781.0  3213820.0   616768.0   \n",
       "43 2014-01-15    mobile   158945.0   391598.0  1022962.0   287547.0   \n",
       "44 2014-01-15    tablet    61490.0    88987.0   214816.0    15967.0   \n",
       "45 2014-01-16   desktop  2134761.0  2489993.0  3522842.0  2030513.0   \n",
       "46 2014-01-16    mobile   339028.0   158945.0   691006.0   321557.0   \n",
       "\n",
       "    expanding_mean_lag1  week  month  day  \n",
       "42         2.018989e+06     3      1   15  \n",
       "43         4.074675e+05     3      1   15  \n",
       "44         1.038917e+05     3      1   15  \n",
       "45         2.050389e+06     3      1   16  \n",
       "46         3.908993e+05     3      1   16  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = mlf.preprocess(data)\n",
    "prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plot_series(prep)\n",
    "fig.savefig('../figs/pipeline_with_sklearn_and_xgboost_random_forest__prep.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/pipeline_with_sklearn_and_xgboost_random_forest__prep.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y                      1.000000\n",
       "lag1                   0.802928\n",
       "lag7                   0.795004\n",
       "lag14                  0.775561\n",
       "expanding_mean_lag1    0.691355\n",
       "week                   0.016375\n",
       "month                  0.015591\n",
       "day                   -0.023339\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.drop(columns=['unique_id', 'ds']).corr()['y']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe the relationship that exists between each day of one of the variables, with this information we can use it to make a decision about what process (lags or lag_transforms) we can use, at the end of the training and predictions of the model we are going to visualize the characteristics more amounts, we will have a little more foundation to be able to add or eliminate those transformations that will be of greater importance for our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fit method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[Pipeline, XGBRegressor], freq=<Day>, lag_features=['lag1', 'lag7', 'lag14', 'expanding_mean_lag1'], date_features=['week', 'month', 'day'], num_threads=6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.fit(data, fitted=True,prediction_intervals=PredictionIntervals(n_windows=5, window_size=30, method=\"conformal_distribution\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predict method with prediction intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>Pipeline</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>Pipeline-lo-95</th>\n",
       "      <th>Pipeline-lo-80</th>\n",
       "      <th>Pipeline-hi-80</th>\n",
       "      <th>Pipeline-hi-95</th>\n",
       "      <th>XGBRegressor-lo-95</th>\n",
       "      <th>XGBRegressor-lo-80</th>\n",
       "      <th>XGBRegressor-hi-80</th>\n",
       "      <th>XGBRegressor-hi-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>16656.18</td>\n",
       "      <td>28669.951172</td>\n",
       "      <td>-154525.58450</td>\n",
       "      <td>-63207.278</td>\n",
       "      <td>96519.638</td>\n",
       "      <td>187837.94450</td>\n",
       "      <td>-161211.866406</td>\n",
       "      <td>-51524.334766</td>\n",
       "      <td>108864.237109</td>\n",
       "      <td>218551.768750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>20496.38</td>\n",
       "      <td>30170.853516</td>\n",
       "      <td>-104696.24700</td>\n",
       "      <td>-73041.798</td>\n",
       "      <td>114034.558</td>\n",
       "      <td>145689.00700</td>\n",
       "      <td>-60974.853906</td>\n",
       "      <td>-58197.007422</td>\n",
       "      <td>118538.714453</td>\n",
       "      <td>121316.560938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>21187.80</td>\n",
       "      <td>29180.378906</td>\n",
       "      <td>14418.49425</td>\n",
       "      <td>14978.427</td>\n",
       "      <td>27397.173</td>\n",
       "      <td>27957.10575</td>\n",
       "      <td>16767.476416</td>\n",
       "      <td>17109.475977</td>\n",
       "      <td>41251.281836</td>\n",
       "      <td>41593.281396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>22426.90</td>\n",
       "      <td>22863.261719</td>\n",
       "      <td>-148873.45925</td>\n",
       "      <td>-73198.757</td>\n",
       "      <td>118052.557</td>\n",
       "      <td>193727.25925</td>\n",
       "      <td>-182880.302344</td>\n",
       "      <td>-86024.932031</td>\n",
       "      <td>131751.455469</td>\n",
       "      <td>228606.825781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>36605.67</td>\n",
       "      <td>18515.167969</td>\n",
       "      <td>-72885.17575</td>\n",
       "      <td>-68990.203</td>\n",
       "      <td>142201.543</td>\n",
       "      <td>146096.51575</td>\n",
       "      <td>-84823.936328</td>\n",
       "      <td>-72305.671094</td>\n",
       "      <td>109336.007031</td>\n",
       "      <td>121854.272266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>tablet</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>332.30</td>\n",
       "      <td>1718.857300</td>\n",
       "      <td>-227611.52400</td>\n",
       "      <td>-224512.356</td>\n",
       "      <td>225176.956</td>\n",
       "      <td>228276.12400</td>\n",
       "      <td>-209634.450122</td>\n",
       "      <td>-206278.888013</td>\n",
       "      <td>209716.602612</td>\n",
       "      <td>213072.164722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>tablet</td>\n",
       "      <td>2019-09-23</td>\n",
       "      <td>356.66</td>\n",
       "      <td>1718.857300</td>\n",
       "      <td>-8433.62200</td>\n",
       "      <td>-8073.658</td>\n",
       "      <td>8786.978</td>\n",
       "      <td>9146.94200</td>\n",
       "      <td>-12324.783765</td>\n",
       "      <td>-11873.774341</td>\n",
       "      <td>15311.488940</td>\n",
       "      <td>15762.498364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>tablet</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>543.04</td>\n",
       "      <td>1718.857300</td>\n",
       "      <td>-230295.58825</td>\n",
       "      <td>-226853.473</td>\n",
       "      <td>227939.553</td>\n",
       "      <td>231381.66825</td>\n",
       "      <td>-265327.840356</td>\n",
       "      <td>-233968.120825</td>\n",
       "      <td>237405.835425</td>\n",
       "      <td>268765.554956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>tablet</td>\n",
       "      <td>2019-09-25</td>\n",
       "      <td>391.96</td>\n",
       "      <td>2976.029541</td>\n",
       "      <td>-259706.93850</td>\n",
       "      <td>-200280.384</td>\n",
       "      <td>201064.304</td>\n",
       "      <td>260490.85850</td>\n",
       "      <td>-217907.899756</td>\n",
       "      <td>-202583.828271</td>\n",
       "      <td>208535.887354</td>\n",
       "      <td>223859.958838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>tablet</td>\n",
       "      <td>2019-09-26</td>\n",
       "      <td>313.98</td>\n",
       "      <td>2976.029541</td>\n",
       "      <td>-7500.99500</td>\n",
       "      <td>-7113.410</td>\n",
       "      <td>7741.370</td>\n",
       "      <td>8128.95500</td>\n",
       "      <td>-10662.555493</td>\n",
       "      <td>-9827.867236</td>\n",
       "      <td>15779.926318</td>\n",
       "      <td>16614.614575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds  Pipeline  XGBRegressor  Pipeline-lo-95  \\\n",
       "0    desktop 2019-08-28  16656.18  28669.951172   -154525.58450   \n",
       "1    desktop 2019-08-29  20496.38  30170.853516   -104696.24700   \n",
       "2    desktop 2019-08-30  21187.80  29180.378906     14418.49425   \n",
       "3    desktop 2019-08-31  22426.90  22863.261719   -148873.45925   \n",
       "4    desktop 2019-09-01  36605.67  18515.167969    -72885.17575   \n",
       "..       ...        ...       ...           ...             ...   \n",
       "85    tablet 2019-09-22    332.30   1718.857300   -227611.52400   \n",
       "86    tablet 2019-09-23    356.66   1718.857300     -8433.62200   \n",
       "87    tablet 2019-09-24    543.04   1718.857300   -230295.58825   \n",
       "88    tablet 2019-09-25    391.96   2976.029541   -259706.93850   \n",
       "89    tablet 2019-09-26    313.98   2976.029541     -7500.99500   \n",
       "\n",
       "    Pipeline-lo-80  Pipeline-hi-80  Pipeline-hi-95  XGBRegressor-lo-95  \\\n",
       "0       -63207.278       96519.638    187837.94450      -161211.866406   \n",
       "1       -73041.798      114034.558    145689.00700       -60974.853906   \n",
       "2        14978.427       27397.173     27957.10575        16767.476416   \n",
       "3       -73198.757      118052.557    193727.25925      -182880.302344   \n",
       "4       -68990.203      142201.543    146096.51575       -84823.936328   \n",
       "..             ...             ...             ...                 ...   \n",
       "85     -224512.356      225176.956    228276.12400      -209634.450122   \n",
       "86       -8073.658        8786.978      9146.94200       -12324.783765   \n",
       "87     -226853.473      227939.553    231381.66825      -265327.840356   \n",
       "88     -200280.384      201064.304    260490.85850      -217907.899756   \n",
       "89       -7113.410        7741.370      8128.95500       -10662.555493   \n",
       "\n",
       "    XGBRegressor-lo-80  XGBRegressor-hi-80  XGBRegressor-hi-95  \n",
       "0        -51524.334766       108864.237109       218551.768750  \n",
       "1        -58197.007422       118538.714453       121316.560938  \n",
       "2         17109.475977        41251.281836        41593.281396  \n",
       "3        -86024.932031       131751.455469       228606.825781  \n",
       "4        -72305.671094       109336.007031       121854.272266  \n",
       "..                 ...                 ...                 ...  \n",
       "85      -206278.888013       209716.602612       213072.164722  \n",
       "86       -11873.774341        15311.488940        15762.498364  \n",
       "87      -233968.120825       237405.835425       268765.554956  \n",
       "88      -202583.828271       208535.887354       223859.958838  \n",
       "89        -9827.867236        15779.926318        16614.614575  \n",
       "\n",
       "[90 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_df = mlf.predict(horizon=30,level=[80,95])\n",
    "forecast_df "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the predictions for the Pipeline (Random Forest) and the XGBRegressor models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plot_series(data, forecast_df, level=[80,95], max_insample_length=100,engine=\"matplotlib\", palette=\"magma\")\n",
    "fig.get_axes()[0].set_title(\"Prediction of Pipeline with intervals\")\n",
    "fig.savefig('../figs/pipeline_with_sklearn_and_xgboost_random_forest__plot_forecasting_pipeline_intervals.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/pipeline_with_sklearn_and_xgboost_random_forest__plot_forecasting_pipeline_intervals.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=pd.Series(mlf.models_['XGBRegressor'].feature_importances_, \n",
    "          index=mlf.ts.features_order_).sort_values(ascending=False).plot.bar(title='Feature Importance XGBRegressor')\n",
    "plt.grid(True)\n",
    "plt.savefig('../figs/pipeline_with_sklearn_and_xgboost_random_forest__plot_feature_importance.png',dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/pipeline_with_sklearn_and_xgboost_random_forest__plot_feature_importance.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate the model’s performance** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "In previous steps, we’ve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\n",
    "\n",
    "With time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\n",
    "\n",
    "The following graph depicts such a Cross Validation Strategy:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Nixtla/statsforecast/main/nbs/imgs/ChainedWindows.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Perform time series cross-validation**\n",
    "\n",
    "In order to get an estimate of how well our model will be when predicting future data we can perform cross validation, which consist on training a few models independently on different subsets of the data, using them to predict a validation set and measuring their performance.\n",
    "\n",
    "Since our data depends on time, we make our splits by removing the last portions of the series and using them as validation sets. This process is implemented in `MLForecast.cross_validation`.\n",
    "\n",
    "Cross-validation of time series models is considered a best practice but most implementations are very slow. The `MlForecast` library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using `Ray, Dask or Spark`.\n",
    "\n",
    "Depending on your computer, this step should take around 1 min.\n",
    "\n",
    "The cross_validation method from the StatsForecast class takes the following arguments.\n",
    "\n",
    "* `df:` training data frame\n",
    "\n",
    "* `h (int):` represents h steps into the future that are being forecasted. In this case, 12 months ahead.\n",
    "\n",
    "* `step_size (int):` step size between each window. In other words: how often do you want to run the forecasting processes.\n",
    "\n",
    "* `n_windows(int):` number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = mlf.cross_validation(\n",
    "    data,\n",
    "    n_windows=5,  # number of models to train/splits to perform\n",
    "    window_size=30,  # length of the validation set in each window\n",
    "    prediction_intervals=PredictionIntervals(n_windows=5, window_size=30, method=\"conformal_distribution\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crossvaldation_df object is a new data frame that includes the following columns:\n",
    "\n",
    "* `unique_id:` index. If you dont like working with index just run `crossvalidation_df.resetindex()`.\n",
    "* `ds:` datestamp or temporal index\n",
    "* `cutoff:` the last datestamp or temporal index for the `n_windows`.\n",
    "* `y:` true value\n",
    "* `model:` columns with the model’s name and fitted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>Pipeline</th>\n",
       "      <th>XGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>134535.0</td>\n",
       "      <td>192685.39</td>\n",
       "      <td>146677.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mobile</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>181469.0</td>\n",
       "      <td>220560.10</td>\n",
       "      <td>226645.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tablet</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>1848.0</td>\n",
       "      <td>7974.42</td>\n",
       "      <td>11840.647461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>581078.0</td>\n",
       "      <td>556486.20</td>\n",
       "      <td>675617.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mobile</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>432169.0</td>\n",
       "      <td>402846.15</td>\n",
       "      <td>353045.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>mobile</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>2019-07-28</td>\n",
       "      <td>4853.0</td>\n",
       "      <td>61083.81</td>\n",
       "      <td>-35539.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>tablet</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>2019-07-28</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2100.08</td>\n",
       "      <td>5553.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>2019-07-28</td>\n",
       "      <td>438.0</td>\n",
       "      <td>63590.21</td>\n",
       "      <td>21381.087891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mobile</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>2019-07-28</td>\n",
       "      <td>557.0</td>\n",
       "      <td>69718.89</td>\n",
       "      <td>-34723.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>tablet</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>2019-07-28</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1883.06</td>\n",
       "      <td>6369.603516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds     cutoff         y   Pipeline   XGBRegressor\n",
       "0    desktop 2019-03-31 2019-03-30  134535.0  192685.39  146677.234375\n",
       "1     mobile 2019-03-31 2019-03-30  181469.0  220560.10  226645.937500\n",
       "2     tablet 2019-03-31 2019-03-30    1848.0    7974.42   11840.647461\n",
       "3    desktop 2019-04-01 2019-03-30  581078.0  556486.20  675617.250000\n",
       "4     mobile 2019-04-01 2019-03-30  432169.0  402846.15  353045.562500\n",
       "..       ...        ...        ...       ...        ...            ...\n",
       "85    mobile 2019-08-26 2019-07-28    4853.0   61083.81  -35539.656250\n",
       "86    tablet 2019-08-26 2019-07-28     199.0    2100.08    5553.020508\n",
       "87   desktop 2019-08-27 2019-07-28     438.0   63590.21   21381.087891\n",
       "88    mobile 2019-08-27 2019-07-28     557.0   69718.89  -34723.070312\n",
       "89    tablet 2019-08-27 2019-07-28      80.0    1883.06    6369.603516\n",
       "\n",
       "[450 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate the model** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "We can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we’ll use the Root Mean Squared Error (RMSE). To do this, we first need to `install datasetsforecast`, a Python library developed **by Nixtla** that includes a function to compute the RMSE.\n",
    "\n",
    "`pip install datasetsforecast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetsforecast.losses import rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to compute the RMSE takes two arguments:\n",
    "\n",
    "1. The actual values.\n",
    "2. The forecasts, in this case,`Pipeline` and `XGBRegressor() Model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetsforecast.losses import mse, mae, rmse\n",
    "\n",
    "def evaluate_cross_validation(df, metric):\n",
    "    models = df.drop(columns=['ds', 'cutoff', 'y']).columns.tolist()\n",
    "    evals = []\n",
    "    for model in models:\n",
    "        eval_ = df.groupby(['unique_id', 'cutoff']).apply(lambda x: metric(x['y'].values, x[model].values)).to_frame() # Calculate loss for every unique_id, model and cutoff.\n",
    "        eval_.columns = [model]\n",
    "        evals.append(eval_)\n",
    "    evals = pd.concat(evals, axis=1)\n",
    "    evals = evals.groupby(['unique_id']).mean(numeric_only=True) # Averages the error metrics for all cutoffs for every combination of model and unique_id\n",
    "    evals['best_model'] = evals.idxmin(axis=1)\n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pipeline</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>best_model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>desktop</th>\n",
       "      <td>84655.000697</td>\n",
       "      <td>75792.889169</td>\n",
       "      <td>XGBRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobile</th>\n",
       "      <td>71030.403796</td>\n",
       "      <td>94662.025775</td>\n",
       "      <td>Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tablet</th>\n",
       "      <td>3852.438835</td>\n",
       "      <td>6845.755177</td>\n",
       "      <td>Pipeline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pipeline  XGBRegressor    best_model\n",
       "unique_id                                          \n",
       "desktop    84655.000697  75792.889169  XGBRegressor\n",
       "mobile     71030.403796  94662.025775      Pipeline\n",
       "tablet      3852.438835   6845.755177      Pipeline"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df = evaluate_cross_validation(cv_result.set_index(\"unique_id\"), rmse)\n",
    "\n",
    "evaluation_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result obtained in this case is given by each type of `unique_ide` identifying the model created with the `RMSE` metric, in which both results are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using cross-validation:  59100.22337380755\n"
     ]
    }
   ],
   "source": [
    "cv_rmse = cv_result.groupby(['unique_id', 'cutoff']).apply(lambda df: rmse(df['y'], df['XGBRegressor'])).mean()\n",
    "print(\"RMSE using cross-validation: \", cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using cross-validation:  53179.28110932804\n"
     ]
    }
   ],
   "source": [
    "cv_rmse = cv_result.groupby(['unique_id', 'cutoff']).apply(lambda df: rmse(df['y'], df['Pipeline'])).mean()\n",
    "print(\"RMSE using cross-validation: \", cv_rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous result, we obtain the `RMSE` metric in a general way without considering the `Unique_id` identifiers, only taking as reference the values obtained from the `Crossvalidation` result, in this case the `Pipeline` and the `XGBRegressor` model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **References** <a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "[Table of Contents](#0)\n",
    "\n",
    "\n",
    "1. Changquan Huang • Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python. \n",
    "2. Ivan Svetunkov. [Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)](https://openforecast.org/adam/)\n",
    "3. [James D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.](https://press.princeton.edu/books/hardcover/9780691042893/time-series-analysis)\n",
    "4. [Nixtla Parameters for Mlforecast](https://nixtla.github.io/mlforecast/forecast.html).\n",
    "5. [Pandas available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "6. [Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Time series cross-validation”.](https://otexts.com/fpp3/tscv.html).\n",
    "7. [Seasonal periods- Rob J Hyndman](https://robjhyndman.com/hyndsight/seasonal-periods/).\n",
    "8. [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlforecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
