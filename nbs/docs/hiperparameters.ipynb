{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of the following article is to obtain a step-by-step guide on how to find the `hyperparameters` of a *time series model with machine learning* using `Mlforecast`.\n",
    "\n",
    "During this walkthrough, we will become familiar with the main `MlForecast` class and some relevant methods such as `mlforecast.fit`, `mlforecast.predict` and `mlforecast.cross_validation` in other.\n",
    "\n",
    "Let's start!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "\n",
    "\n",
    "\n",
    "1.\t[Introduction](#1)\n",
    "2.\t[What is Optuna?](#2)\n",
    "3.\t[Loading libraries and data](#3)\n",
    "4.\t[Explore Data with the plot method](#4)\n",
    "5.\t[Implementation of model with MLForecast](#5)\n",
    "6.  [How To Tune XGBoost Hyperparameters With Optuna](#6)\n",
    "7.  [References](#7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "When designing Machine Learning models, model optimization is always a very important issue. It is a generally very tedious process that is usually left at the very end of the production cycle due to the amount of time it requires, however, in this notebook we are going to focus on being able to find that hyperparameter of our machine learning model for series of time.\n",
    "\n",
    "In machine learning, hyperparameters are fixed parameters that have been assigned before training, not identical to other parameters in a model.\n",
    "\n",
    "Hyperparameters are made up of parts of the training algorithm and would characterize the architecture of a model. It consists of an improved pipeline that can greatly affect the efficiency of the model.\n",
    "\n",
    "Addressing the need for this attribute and in terms of analyzing such specific algorithms and models that need fine tuning, we will use Optuna which has features that facilitate this problem by regulating the hyperparameter settings.\n",
    "\n",
    "These processes are actually nothing more than choosing a series of values for each hyperparameter, making the possible combinations and starting to test different values. Some methods perform an exhaustive search and others choose only certain elements of the search space. They are very effective methods and generally allow us to obtain the optimal (quasi-optimal) hyperparameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What is Optuna?** <a class=\"anchor\" id=\"2\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Optuna is an automated hyperparameter optimization software framework that was knowingly invented for machine learning-based tasks. Emphasizes a run-defined authoritative approach user API.\n",
    "\n",
    "Due to the run-defined API, code script written with Optuna retains extreme modularity, and Optuna users could actively compose “search spaces” for hyperparameters.\n",
    "\n",
    "Optuna is a software framework for automated hyperparameter optimization procedure. By the known fact, it inspects and identifies optimal hyperparameter values through trial and error method for efficient performance and high efficiency.\n",
    "\n",
    "Optuna emerges as a hyperparameter optimization software under a new design criterion that is based on three fundamental ideas:\n",
    "\n",
    "- define-by-run API that allows users to dynamically build and manipulate search spaces,\n",
    "- efficient implementation that focuses on the optimal functionality of sampling strategies as well as pruning algorithms, and\n",
    "- easy to configure that focuses on versatility, that is, it allows optimizing functions in lightweight environments as well as large-scale experiments in environments based on distributed and parallel computing.\n",
    "\n",
    "The criteria on which Optuna is designed make it easy to implement, flexible and scalable. Due to the scalability property of Optuna, optimization of large-scale experiments can be performed in a parallel and distributed manner. Optuna is framework agnostic, that is, it can be easily integrated with any of the machine learning and deep learning frameworks such as: PyTorch, Tensorflow, Keras, Scikit-Learn, XGBoost, etc.\n",
    "\n",
    "### **What is a hyperparameter?**\n",
    "\n",
    "A hyperparameter is a parameter to control how a machine learning algorithm behaves. In deep learning, the learning rate, batch size, and number of training iterations are hyperparameters. Hyperparameters also include the number of layers and channels of the neural network. They are not, however, just numerical values. Things like using Momentum SGD or Adam in training are also considered hyperparameters.\n",
    "\n",
    "It is almost impossible to get a machine learning algorithm to do the job without tuning the hyperparameters. The number of hyperparameters tends to be high, especially in deep learning, and performance is thought to largely depend on how we tune them. Most researchers and engineers using deep learning technology manually tune these hyperparameters and spend a significant amount of their time doing so."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optimal Strategy for Optimization**\n",
    "\n",
    "Optuna generally uses the following strategy to find the best combination of hyperparameters.\n",
    "\n",
    "### **Test strategy**\n",
    "\n",
    "It uses a testing algorithm to select the best combination of parameters from a list of all possible combinations. It focuses on areas where hyperparameters are giving good results and ignores others, resulting in time savings.\n",
    "\n",
    "Optuna allows you to build and manipulate hyperparameter search spaces dynamically. To sample settings from the search space, Optuna provides two types of photos:\n",
    "\n",
    "- Relational sampling: this type of methods take into account information about the consequence between the parameters.\n",
    "- Independent sampling.\n",
    "\n",
    "The Tree-structured Parzen Estimator (TPE) is the default sampler in Optuna. It uses the history of previously evaluated hyperparameter configurations to sample the following ones.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pruning strategy**\n",
    "\n",
    "It uses a pruning strategy that constantly checks the performance of the algorithm during training and prunes (terminates) the training for a particular combination of hyperparameters if it is not giving good results. This also results in time savings.\n",
    "\n",
    "A pruning mechanism refers to the termination of unpromising tests during hyperparameter optimization. Periodically monitor the learning curves of each test. Next, determine the sets of hyperparameters that will not lead to a good result and should not be considered.\n",
    "\n",
    "The pruning mechanism implemented in Optuna is based on an asynchronous variant of the successive halving algorithm (SHA). Let's understand the general idea behind SHA:\n",
    "\n",
    "- Assign the minimum amount of resources to each available hyperparameter configuration. The resources, for example, are the number of epochs, the number of training examples, the training duration, etc.\n",
    "- Evaluate performance metrics of all configurations within allocated resources.\n",
    "- Keep the upper settings 1/ η (η – a reduction factor) with the best pressures and discard the rest.\n",
    "- Increase the minimum amount of resources per configuration by the factor η and repeat until the amount of resources per configuration reaches the maximum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading libraries and data** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling and processing of Data\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Handling and processing of Data for Date (time)\n",
    "# ==============================================================================\n",
    "import datetime\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# \n",
    "# ==============================================================================\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "# \n",
    "# ==============================================================================\n",
    "from utilsforecast.plotting import plot_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# \n",
    "# ==============================================================================\n",
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "from window_ops.ewm import ewm_mean\n",
    "from mlforecast.target_transforms import Differences\n",
    "\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plt.style.use('grayscale') # fivethirtyeight  grayscale  classic\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "dark_style = {\n",
    "    'figure.facecolor': '#008080',  # #212946\n",
    "    'axes.facecolor': '#008080',\n",
    "    'savefig.facecolor': '#008080',\n",
    "    'axes.grid': True,\n",
    "    'axes.grid.which': 'both',\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'grid.color': '#000000',  #2A3459\n",
    "    'grid.linewidth': '1',\n",
    "    'text.color': '0.9',\n",
    "    'axes.labelcolor': '0.9',\n",
    "    'xtick.color': '0.9',\n",
    "    'ytick.color': '0.9',\n",
    "    'font.size': 12 }\n",
    "plt.rcParams.update(dark_style)\n",
    "# Define the plot size\n",
    "# ==============================================================================\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18,7)\n",
    "\n",
    "# Hide warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>semanas</th>\n",
       "      <th>malaria_vivax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-12-31</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-01-07</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-01-14</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-01-21</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-01-28</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     semanas  malaria_vivax\n",
       "0 2007-12-31           61.0\n",
       "1 2008-01-07           87.0\n",
       "2 2008-01-14           64.0\n",
       "3 2008-01-21           69.0\n",
       "4 2008-01-28           54.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"https://raw.githubusercontent.com/Naren8520/Serie-de-tiempo-con-Machine-Learning/main/Data/tipos_malarias_choco_colombia.csv\",parse_dates=[\"semanas\"],sep=\";\",usecols=[0,3] )\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to MlForecast is always a data frame in long format with three columns: unique_id, ds and y:\n",
    "\n",
    "* The `unique_id` (string, int or category) represents an identifier for the series.\n",
    "\n",
    "* The `ds` (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n",
    "\n",
    "* The `y` (numeric) represents the measurement we wish to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-12-31</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-01-07</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-01-14</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-01-21</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-01-28</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ds     y unique_id\n",
       "0 2007-12-31  61.0         1\n",
       "1 2008-01-07  87.0         1\n",
       "2 2008-01-14  64.0         1\n",
       "3 2008-01-21  69.0         1\n",
       "4 2008-01-28  54.0         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df[\"unique_id\"]=\"1\"\n",
    "df.columns=[\"ds\", \"y\", \"unique_id\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 782 entries, 0 to 782\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   ds         782 non-null    datetime64[ns]\n",
      " 1   y          782 non-null    float64       \n",
      " 2   unique_id  782 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 24.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Explore Data with the plot method** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Plot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_series(df)\n",
    "fig.savefig('../figs/hiperparameters__eda.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/hiperparameters__eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Augmented Dickey-Fuller Test**\n",
    "An Augmented Dickey-Fuller (ADF) test is a type of statistical test that determines whether a unit root is present in time series data. Unit roots can cause unpredictable results in time series analysis. A null hypothesis is formed in the unit root test to determine how strongly time series data is affected by a trend. By accepting the null hypothesis, we accept the evidence that the time series data is not stationary. By rejecting the null hypothesis or accepting the alternative hypothesis, we accept the evidence that the time series data is generated by a stationary process. This process is also known as stationary trend. The values of the ADF test statistic are negative. Lower ADF values indicate a stronger rejection of the null hypothesis.\n",
    "\n",
    "Augmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\n",
    "\n",
    "- Null Hypothesis: Time Series is non-stationary. It gives a time-dependent trend.\n",
    "- Alternate Hypothesis: Time Series is stationary. In another term, the series doesn’t depend on time.\n",
    "\n",
    "- ADF or t Statistic < critical values: Reject the null hypothesis, time series is stationary.\n",
    "- ADF or t Statistic > critical values: Failed to reject the null hypothesis, time series is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_dickey_fuller_test(series , column_name):\n",
    "    print (f'Dickey-Fuller test results for columns: {column_name}')\n",
    "    dftest = adfuller(series, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n",
    "    for key,value in dftest[4].items():\n",
    "       dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "    if dftest[1] <= 0.05:\n",
    "        print(\"Conclusion:====>\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"The data is stationary\")\n",
    "    else:\n",
    "        print(\"Conclusion:====>\")\n",
    "        print(\"The null hypothesis cannot be rejected\")\n",
    "        print(\"The data is not stationary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dickey-Fuller test results for columns: Vivax Malaria\n",
      "Test Statistic                  -3.961578\n",
      "p-value                          0.001626\n",
      "No Lags Used                    13.000000\n",
      "Number of observations used    768.000000\n",
      "Critical Value (1%)             -3.438893\n",
      "Critical Value (5%)             -2.865311\n",
      "Critical Value (10%)            -2.568778\n",
      "dtype: float64\n",
      "Conclusion:====>\n",
      "Reject the null hypothesis\n",
      "The data is stationary\n"
     ]
    }
   ],
   "source": [
    "augmented_dickey_fuller_test(df[\"y\"],\"Vivax Malaria\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Autocorrelation plots**\n",
    "\n",
    "### **Autocorrelation Function**\n",
    "\n",
    "**Definition 1.** Let $\\{x_t;1 ≤ t ≤ n\\}$ be a time series sample of size n from $\\{X_t\\}$.\n",
    "1. $\\bar x = \\sum_{t=1}^n \\frac{x_t}{n}$ is called the sample mean of $\\{X_t\\}$.\n",
    "2. $c_k =\\sum_{t=1}^{n−k} (x_{t+k}- \\bar x)(x_t−\\bar x)/n$ is known as the sample autocovariance function of $\\{X_t\\}$.\n",
    "3. $r_k = c_k /c_0$ is said to be the sample autocorrelation function of $\\{X_t\\}$. \n",
    "\n",
    "Note the following remarks about this definition:\n",
    " \n",
    "* Like most literature, this guide uses ACF to denote the sample autocorrelation function as well as the autocorrelation function. What is denoted by ACF can easily be identified in context.\n",
    "\n",
    "* Clearly c0 is the sample variance of $\\{X_t\\}$. Besides, $r_0 = c_0/c_0 = 1$ and for any integer $k, |r_k| ≤ 1$.\n",
    "\n",
    "* When we compute the ACF of any sample series with a fixed length $n$, we cannot put too much confidence in the values of $r_k$ for large k’s, since fewer pairs of $(x_{t +k }, x_t )$ are available for calculating $r_k$ as $k$ is large. One rule of thumb is not to estimate $r_k$ for $k > n/3$, and another is $n ≥ 50, k ≤ n/4$. In any case, it is always a good idea to be careful.\n",
    "\n",
    "* We also compute the ACF of a nonstationary time series sample by Definition 1. In this case, however, the ACF or $r_k$ very slowly or hardly tapers off as $k$ increases.\n",
    "\n",
    "* Plotting the ACF $(r_k)$ against lag $k$ is easy but very helpful in analyzing time series sample. Such an ACF plot is known as a correlogram.\n",
    "\n",
    "* If $\\{X_t\\}$ is stationary with $E(X_t)=0$ and $\\rho_k =0$ for all $k \\neq 0$,thatis,itisa white noise series, then the sampling distribution of $r_k$ is asymptotically normal with the mean 0 and the variance of $1/n$. Hence, there is about 95% chance that $r_k$ falls in the interval $[−1.96/√n, 1.96/√n]$.\n",
    "\n",
    "Now we can give a summary that (1) if the time series plot of a time series clearly shows a trend or/and seasonality, it is surely nonstationary; (2) if the ACF $r_k$ very slowly or hardly tapers off as lag $k$ increases, the time series should also be nonstationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "plot_acf(df[\"y\"],  lags=30, ax=axs[0],color=\"fuchsia\")\n",
    "axs[0].set_title(\"Autocorrelation\");\n",
    "\n",
    "# Grafico\n",
    "plot_pacf(df[\"y\"],  lags=30, ax=axs[1],color=\"lime\")\n",
    "axs[1].set_title('Partial Autocorrelation')\n",
    "plt.savefig(\"../figs/hiperparameters__autocorrelation.png\")\n",
    "plt.close();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/hiperparameters__autocorrelation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decomposition of the time series**\n",
    "\n",
    "How to decompose a time series and why?\n",
    "\n",
    "In time series analysis to forecast new values, it is very important to know past data. More formally, we can say that it is very important to know the patterns that values follow over time. There can be many reasons that cause our forecast values to fall in the wrong direction. Basically, a time series consists of four components. The variation of those components causes the change in the pattern of the time series. These components are:\n",
    "\n",
    "* **Level:** This is the primary value that averages over time.\n",
    "* **Trend:** The trend is the value that causes increasing or decreasing patterns in a time series.\n",
    "* **Seasonality:** This is a cyclical event that occurs in a time series for a short time and causes short-term increasing or decreasing patterns in a time series.\n",
    "* **Residual/Noise:** These are the random variations in the time series.\n",
    "\n",
    "Combining these components over time leads to the formation of a time series. Most time series consist of level and noise/residual and trend or seasonality are optional values.\n",
    "\n",
    "If seasonality and trend are part of the time series, then there will be effects on the forecast value. As the pattern of the forecasted time series may be different from the previous time series.\n",
    "\n",
    "The combination of the components in time series can be of two types:\n",
    "* Additive\n",
    "* multiplicative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = seasonal_decompose(df[\"y\"], model = \"additive\", period=24).plot()\n",
    "a.savefig('../figs/hiperparameters__seasonal_decompose_aditive.png')\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/hiperparameters__seasonal_decompose_aditive.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = seasonal_decompose(df[\"y\"], model = \"Multiplicative\", period=24).plot()\n",
    "m.savefig('../figs/hiperparameters__seasonal_decompose_multiplicative.png')\n",
    "plt.close();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/hiperparameters__seasonal_decompose_multiplicative.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling with MLForecast** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building Model**\n",
    "\n",
    "We define the model that we want to use, for our example we are going to use the `XGBoost model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [XGBRegressor(n_estimators=100,                              \n",
    "                               random_state=1234,\n",
    "                               n_jobs=-1) ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the models by instantiating a new `MlForecast` object with the following parameters:\n",
    "\n",
    "* `models:` a list of models. Select the models you want from models and import them.\n",
    "\n",
    "* `freq:` a string indicating the frequency of the data. (See [panda’s available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).)\n",
    "\n",
    "* `lags:` Lags of the target to uses as feature.\n",
    "\n",
    "* `lag_transforms:` Mapping of target lags to their transformations.\n",
    "\n",
    "* `date_features:` Features computed from the dates. Can be `pandas` date attributes or functions that will take the dates as input.\n",
    "\n",
    "* `differences:` Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "\n",
    "* `num_threads:` Number of threads to use when computing the features.\n",
    "\n",
    "* `target_transforms:` Transformations that will be applied to the target computing the features and restored after the forecasting step.\n",
    "\n",
    "Any settings are passed into the constructor. Then you call its fit method and pass in the historical data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_day(dates):\n",
    "    return dates.day % 2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MLForecast(models=model,\n",
    "                 freq='W', \n",
    "                 lags=[1,7,14],\n",
    "                 lag_transforms={1: [expanding_mean], 12: [(rolling_mean, 7)] },\n",
    "                 date_features=['dayofweek', 'month', even_day],\n",
    "                 num_threads=2,        \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>lag1</th>\n",
       "      <th>lag7</th>\n",
       "      <th>lag14</th>\n",
       "      <th>expanding_mean_lag1</th>\n",
       "      <th>rolling_mean_lag12_window_size7</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>even_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2008-05-05</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>63.888889</td>\n",
       "      <td>64.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2008-05-12</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>95.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>65.526316</td>\n",
       "      <td>67.428571</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2008-05-19</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>66.050000</td>\n",
       "      <td>64.571429</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2008-05-26</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>65.952381</td>\n",
       "      <td>63.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2008-06-02</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1</td>\n",
       "      <td>114.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>68.136364</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1</td>\n",
       "      <td>186.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>154.800515</td>\n",
       "      <td>222.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1</td>\n",
       "      <td>143.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>154.785347</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>2022-12-12</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1</td>\n",
       "      <td>155.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>154.785623</td>\n",
       "      <td>204.428571</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "      <td>134.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>154.758974</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>2022-12-26</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>154.692702</td>\n",
       "      <td>191.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds      y unique_id   lag1   lag7  lag14  expanding_mean_lag1  \\\n",
       "18  2008-05-05   95.0         1   52.0   42.0   54.0            63.888889   \n",
       "19  2008-05-12   76.0         1   95.0   64.0   58.0            65.526316   \n",
       "20  2008-05-19   64.0         1   76.0   76.0   56.0            66.050000   \n",
       "21  2008-05-26  114.0         1   64.0   80.0   84.0            65.952381   \n",
       "22  2008-06-02  150.0         1  114.0   60.0   67.0            68.136364   \n",
       "..         ...    ...       ...    ...    ...    ...                  ...   \n",
       "778 2022-11-28  143.0         1  186.0  161.0  207.0           154.800515   \n",
       "779 2022-12-05  155.0         1  143.0  195.0  236.0           154.785347   \n",
       "780 2022-12-12  134.0         1  155.0  186.0  206.0           154.785623   \n",
       "781 2022-12-19  103.0         1  134.0  168.0  150.0           154.758974   \n",
       "782 2022-12-26   38.0         1  103.0  153.0  177.0           154.692702   \n",
       "\n",
       "     rolling_mean_lag12_window_size7  dayofweek  month  even_day  \n",
       "18                         64.142857          0      5     False  \n",
       "19                         67.428571          0      5      True  \n",
       "20                         64.571429          0      5     False  \n",
       "21                         63.142857          0      5      True  \n",
       "22                         59.000000          0      6      True  \n",
       "..                               ...        ...    ...       ...  \n",
       "778                       222.285714          0     11      True  \n",
       "779                       217.000000          0     12     False  \n",
       "780                       204.428571          0     12      True  \n",
       "781                       196.000000          0     12     False  \n",
       "782                       191.285714          0     12      True  \n",
       "\n",
       "[764 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = mlf.preprocess(df)\n",
    "prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fit method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[XGBRegressor], freq=<Week: weekday=6>, lag_features=['lag1', 'lag7', 'lag14', 'expanding_mean_lag1', 'rolling_mean_lag12_window_size7'], date_features=['dayofweek', 'month', <function even_day at 0x16283c310>], num_threads=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the models\n",
    "mlf.fit(df,  \n",
    " fitted=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results of our model in this case the `XGBRegressor model`. We can observe it with the following instruction:\n",
    "\n",
    "Let us now visualize the fitted values of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>XGBRegressor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-05-05</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.297630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-05-12</td>\n",
       "      <td>76.0</td>\n",
       "      <td>78.370583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-05-19</td>\n",
       "      <td>64.0</td>\n",
       "      <td>67.504013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-05-26</td>\n",
       "      <td>114.0</td>\n",
       "      <td>112.061485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-06-02</td>\n",
       "      <td>150.0</td>\n",
       "      <td>147.969345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>143.0</td>\n",
       "      <td>143.187683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>155.0</td>\n",
       "      <td>155.217651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-12</td>\n",
       "      <td>134.0</td>\n",
       "      <td>133.981125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>103.0</td>\n",
       "      <td>104.260315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-26</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.831478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds      y  XGBRegressor\n",
       "unique_id                                \n",
       "1         2008-05-05   95.0     94.297630\n",
       "1         2008-05-12   76.0     78.370583\n",
       "1         2008-05-19   64.0     67.504013\n",
       "1         2008-05-26  114.0    112.061485\n",
       "1         2008-06-02  150.0    147.969345\n",
       "...              ...    ...           ...\n",
       "1         2022-11-28  143.0    143.187683\n",
       "1         2022-12-05  155.0    155.217651\n",
       "1         2022-12-12  134.0    133.981125\n",
       "1         2022-12-19  103.0    104.260315\n",
       "1         2022-12-26   38.0     38.831478\n",
       "\n",
       "[764 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=mlf.forecast_fitted_values()\n",
    "result=result.set_index(\"unique_id\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import normal_ad\n",
    "from scipy import stats\n",
    "\n",
    "sw_result = stats.shapiro(result[\"XGBRegressor\"])\n",
    "ad_result = normal_ad(np.array(result[\"XGBRegressor\"]), axis=0)\n",
    "dag_result = stats.normaltest(result[\"XGBRegressor\"], axis=0, nan_policy='propagate')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that we can only use this method if we assume that the residuals of our validation predictions are normally distributed. To see if this is the case, we will use a PP-plot and test its normality with the Anderson-Darling, Kolmogorov-Smirnov, and D’Agostino K^2 tests.\n",
    "\n",
    "The PP-plot(Probability-to-Probability) plots the data sample against the normal distribution plot in such a way that if normally distributed, the data points will form a straight line.\n",
    "\n",
    "The three normality tests determine how likely a data sample is from a normally distributed population using p-values. The null hypothesis for each test is that \"the sample came from a normally distributed population\". This means that if the resulting p-values are below a chosen alpha value, then the null hypothesis is rejected. Thus there is evidence to suggest that the data comes from a non-normal distribution. For this article, we will use an Alpha value of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=mlf.forecast_fitted_values()\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "# plot[1,1]\n",
    "result[\"XGBRegressor\"].plot(ax=axs[0,0])\n",
    "axs[0,0].set_title(\"Residuals model\");\n",
    "\n",
    "# plot\n",
    "#plot(result[\"XGBRegressor\"], ax=axs[0,1]);\n",
    "axs[0,1].hist(result[\"XGBRegressor\"], density=True,bins=50, alpha=0.5 )\n",
    "axs[0,1].set_title(\"Density plot - Residual\");\n",
    "\n",
    "# plot\n",
    "stats.probplot(result[\"XGBRegressor\"], dist=\"norm\", plot=axs[1,0])\n",
    "axs[1,0].set_title('Plot Q-Q')\n",
    "axs[1,0].annotate(\"SW p-val: {:.4f}\".format(sw_result[1]), xy=(0.05,0.9), xycoords='axes fraction', fontsize=15,\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\", pad=0.6))\n",
    "\n",
    "axs[1,0].annotate(\"AD p-val: {:.4f}\".format(ad_result[1]), xy=(0.05,0.8), xycoords='axes fraction', fontsize=15,\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\", pad=0.6))\n",
    "\n",
    "axs[1,0].annotate(\"DAG p-val: {:.4f}\".format(dag_result[1]), xy=(0.05,0.7), xycoords='axes fraction', fontsize=15,\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\", pad=0.6))\n",
    "# plot\n",
    "plot_acf(result[\"XGBRegressor\"],  lags=35, ax=axs[1,1],color=\"fuchsia\")\n",
    "axs[1,1].set_title(\"Autocorrelation\");\n",
    "\n",
    "plt.savefig(\"../figs/hiperparameters__plot_residual_model.png\")\n",
    "plt.close();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/hiperparameters__plot_residual_model.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predict method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>XGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>136.666107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>206.950119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>201.778046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>202.578064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>205.729736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>183.612137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-13</td>\n",
       "      <td>172.488846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-20</td>\n",
       "      <td>227.618134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>225.309097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>235.971085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>231.503281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>231.228714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>206.249329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>148.997559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>229.750259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>238.120422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>215.677734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>241.779907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>202.084854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>219.726471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>210.397415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-29</td>\n",
       "      <td>248.285828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-05</td>\n",
       "      <td>211.757706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>203.184540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-19</td>\n",
       "      <td>195.601807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-26</td>\n",
       "      <td>173.735474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-03</td>\n",
       "      <td>215.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-10</td>\n",
       "      <td>200.695709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>190.196823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>180.168762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds  XGBRegressor\n",
       "0          1 2023-01-02    136.666107\n",
       "1          1 2023-01-09    206.950119\n",
       "2          1 2023-01-16    201.778046\n",
       "3          1 2023-01-23    202.578064\n",
       "4          1 2023-01-30    205.729736\n",
       "5          1 2023-02-06    183.612137\n",
       "6          1 2023-02-13    172.488846\n",
       "7          1 2023-02-20    227.618134\n",
       "8          1 2023-02-27    225.309097\n",
       "9          1 2023-03-06    235.971085\n",
       "10         1 2023-03-13    231.503281\n",
       "11         1 2023-03-20    231.228714\n",
       "12         1 2023-03-27    206.249329\n",
       "13         1 2023-04-03    148.997559\n",
       "14         1 2023-04-10    229.750259\n",
       "15         1 2023-04-17    238.120422\n",
       "16         1 2023-04-24    215.677734\n",
       "17         1 2023-05-01    241.779907\n",
       "18         1 2023-05-08    202.084854\n",
       "19         1 2023-05-15    219.726471\n",
       "20         1 2023-05-22    210.397415\n",
       "21         1 2023-05-29    248.285828\n",
       "22         1 2023-06-05    211.757706\n",
       "23         1 2023-06-12    203.184540\n",
       "24         1 2023-06-19    195.601807\n",
       "25         1 2023-06-26    173.735474\n",
       "26         1 2023-07-03    215.806900\n",
       "27         1 2023-07-10    200.695709\n",
       "28         1 2023-07-17    190.196823\n",
       "29         1 2023-07-24    180.168762"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "forecast_df = mlf.predict(h=30) \n",
    "forecast_df[\"ds\"]=forecast_df[\"ds\"]+timedelta(days=1)\n",
    "forecast_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plot_series(df, forecast_df, max_insample_length=500,engine=\"matplotlib\")\n",
    "for ax in fig.get_axes():\n",
    "   ax.set_title(\"Prediction\")\n",
    "fig.savefig('../figs/hiperparameters__plot_forecasting_intervals.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/hiperparameters__plot_forecasting_intervals.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the feature importances too.\n",
    "\n",
    "This model is heavily dependent on the lag 1 feature, as you can see in the feature importance plot for `XGBRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=pd.Series(mlf.models_['XGBRegressor'].feature_importances_, \n",
    "              index=mlf.ts.features_order_).sort_values(ascending=False).plot.bar(title='Feature Importance XGBRegressor')\n",
    "plt.savefig('../figs/hiperparameters__plot_feature_importance.png',dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/hiperparameters__plot_feature_importance.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How To Tune XGBoost Hyperparameters With Optuna** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Table of Contents](#0)\n",
    "\n",
    "Let’s use Optuna which is a straightforward hyperparameter tuning library in Python.\n",
    "\n",
    "We need to pack our code inside an objective function that takes the trial object as an argument.\n",
    "\n",
    "This object encapsulates one step of the optimization process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split the data into training and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df.ds<='2022-05-30'] \n",
    "test = df[df.ds>'2022-05-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((752, 3), (30, 3))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize XGBoost hyperparameters with optuna\n",
    "def objective(trial):\n",
    "    # create the regressor object\n",
    "    lags = trial.suggest_int('lags', 14, 56, step=7)\n",
    "    regressor = [XGBRegressor(n_estimators=trial.suggest_int('n_estimators', 70, 1000),\n",
    "                             max_depth=trial.suggest_int('max_depth', 2, 6),\n",
    "                             min_child_weight=trial.suggest_float('min_child_weight', 0, 6),\n",
    "                             gamma=trial.suggest_float('gamma', 0.001, 6, log=True),\n",
    "                             learning_rate=trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "                             subsample=trial.suggest_float('subsample', 0.50, 1),\n",
    "                             colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "                             reg_lambda=trial.suggest_float('reg_lambda', 0.001, 5, log=True))]\n",
    "    model = MLForecast(models=regressor,\n",
    "                    freq='W',\n",
    "                    lags=[1,7, lags],\n",
    "                    lag_transforms={1: [expanding_mean], 12: [(rolling_mean, 7)] },)\n",
    "    # fit model\n",
    "    model.fit(train)\n",
    "    # predict model\n",
    "    p = model.predict(horizon=30)\n",
    "    p[\"ds\"]=p[\"ds\"]+timedelta(days=1)\n",
    "    p = p.merge(test, on=['unique_id', 'ds'], how='left')\n",
    "\n",
    "    error = mean_absolute_percentage_error(p['y'], p['XGBRegressor'])\n",
    "\n",
    "    return error\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each hyperparameter we want to tune, we use the `trial.suggest_*` methods.\n",
    "\n",
    "This will sample a value for the hyperparameter from the specified distribution with different strategies that we can choose.\n",
    "\n",
    "For now I will use the default strategy which is `Tree-structured Parzen Estimator`.\n",
    "\n",
    "The ranges we picked for each hyperparameter are based on every that own experience with XGBoost\n",
    "\n",
    "I added a `lags hyperparameter` to show that you can tune any number that is used in your code.\n",
    "\n",
    "We are tuning the longest lag we are using, but we could tune everything, even the window sizes of the rolling transforms or even which transforms to use.\n",
    "\n",
    "max_depth and min_child_weight control how deep (and complex) each tree can be.\n",
    "\n",
    "Deeper trees can capture more complex patterns, but they are more likely to overfit.\n",
    "\n",
    "On the other hand, limiting the number of samples in each leaf node can help prevent overfitting because the tree can’t grow unless it has enough samples to split.\n",
    "\n",
    "`subsample` and `colsample_bytree` define what percentage of the samples and features to randomly sample before building each tree.\n",
    "\n",
    "This is a very good way to prevent overfitting.\n",
    "\n",
    "After we define the objective function, we can create the study object and pass the objective function to it.\n",
    "\n",
    "We're going to try it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-19 15:43:13,246] A new study created in memory with name: no-name-352e7b9d-fde2-438c-8cc4-282cea974102\n",
      "[I 2023-09-19 15:43:13,611] Trial 0 finished with value: 0.6905306474799863 and parameters: {'lags': 14, 'n_estimators': 469, 'max_depth': 5, 'min_child_weight': 4.50163860564606, 'gamma': 0.013960193439367, 'learning_rate': 0.0013305504170062963, 'subsample': 0.8811803896252044, 'colsample_bytree': 0.5303358418038833, 'reg_lambda': 4.346896101328735}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:14,028] Trial 1 finished with value: 0.38221777561508335 and parameters: {'lags': 21, 'n_estimators': 940, 'max_depth': 2, 'min_child_weight': 0.6102551304950998, 'gamma': 4.227154581592189, 'learning_rate': 0.07869555180432371, 'subsample': 0.7241820946807263, 'colsample_bytree': 0.7742561356512968, 'reg_lambda': 0.4348471621028174}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:14,852] Trial 2 finished with value: 0.4342010164034546 and parameters: {'lags': 56, 'n_estimators': 787, 'max_depth': 6, 'min_child_weight': 4.861777601368881, 'gamma': 4.859457819589681, 'learning_rate': 0.10810472068865092, 'subsample': 0.865183121872692, 'colsample_bytree': 0.9477402452652808, 'reg_lambda': 0.03792426112951265}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:15,386] Trial 3 finished with value: 0.2763113106066201 and parameters: {'lags': 49, 'n_estimators': 685, 'max_depth': 4, 'min_child_weight': 5.807311013907121, 'gamma': 0.23368991340346043, 'learning_rate': 0.003748986567577188, 'subsample': 0.7750360615708931, 'colsample_bytree': 0.99241938551464, 'reg_lambda': 1.2841613746944749}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:15,513] Trial 4 finished with value: 0.3339447265846935 and parameters: {'lags': 49, 'n_estimators': 139, 'max_depth': 2, 'min_child_weight': 1.5105780441573762, 'gamma': 0.004897509599509092, 'learning_rate': 0.02258509970495683, 'subsample': 0.760218569919188, 'colsample_bytree': 0.8694810800778265, 'reg_lambda': 0.07048148437596616}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:15,890] Trial 5 finished with value: 0.4208916448312081 and parameters: {'lags': 21, 'n_estimators': 522, 'max_depth': 4, 'min_child_weight': 3.886825180328244, 'gamma': 0.0011048876530697616, 'learning_rate': 0.02270045522342982, 'subsample': 0.8679933765945493, 'colsample_bytree': 0.588007835522597, 'reg_lambda': 0.006102297943716127}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:16,095] Trial 6 finished with value: 0.43456898814324724 and parameters: {'lags': 21, 'n_estimators': 210, 'max_depth': 4, 'min_child_weight': 4.33115302767489, 'gamma': 0.005662988180463128, 'learning_rate': 0.17683338447529173, 'subsample': 0.9803952123993416, 'colsample_bytree': 0.8099669384097268, 'reg_lambda': 0.037895259641779616}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:16,730] Trial 7 finished with value: 0.5746996632010253 and parameters: {'lags': 28, 'n_estimators': 837, 'max_depth': 4, 'min_child_weight': 0.910971103692227, 'gamma': 0.2760979969438713, 'learning_rate': 0.0016161258492586553, 'subsample': 0.8247176304271711, 'colsample_bytree': 0.9988357080422615, 'reg_lambda': 0.023118735350009093}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:17,537] Trial 8 finished with value: 0.413080073897608 and parameters: {'lags': 56, 'n_estimators': 877, 'max_depth': 5, 'min_child_weight': 1.0985157517247441, 'gamma': 0.7741014617230969, 'learning_rate': 0.007749845906654026, 'subsample': 0.5786400841467667, 'colsample_bytree': 0.9545098487934276, 'reg_lambda': 1.3810168183320155}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:18,093] Trial 9 finished with value: 0.3805472600711353 and parameters: {'lags': 21, 'n_estimators': 574, 'max_depth': 5, 'min_child_weight': 5.847584926420152, 'gamma': 0.04124575175747955, 'learning_rate': 0.01126131915927252, 'subsample': 0.637963582740632, 'colsample_bytree': 0.8746810177050707, 'reg_lambda': 1.689165925777164}. Best is trial 0 with value: 0.6905306474799863.\n",
      "[I 2023-09-19 15:43:18,382] Trial 10 finished with value: 0.8382821227901963 and parameters: {'lags': 35, 'n_estimators': 327, 'max_depth': 6, 'min_child_weight': 2.8920795213931583, 'gamma': 0.028281491055029515, 'learning_rate': 0.00110448406926732, 'subsample': 0.9936816113473002, 'colsample_bytree': 0.5007754492311655, 'reg_lambda': 4.2673805851616295}. Best is trial 10 with value: 0.8382821227901963.\n",
      "[I 2023-09-19 15:43:18,722] Trial 11 finished with value: 0.8405989621214335 and parameters: {'lags': 35, 'n_estimators': 353, 'max_depth': 6, 'min_child_weight': 2.9149785262820016, 'gamma': 0.03289421276419349, 'learning_rate': 0.0010025781280307772, 'subsample': 0.9718240942959773, 'colsample_bytree': 0.5043502636332999, 'reg_lambda': 4.50596376360683}. Best is trial 11 with value: 0.8405989621214335.\n",
      "[I 2023-09-19 15:43:19,177] Trial 12 finished with value: 0.8281913079826426 and parameters: {'lags': 35, 'n_estimators': 349, 'max_depth': 6, 'min_child_weight': 2.5868777152395657, 'gamma': 0.06089237472531987, 'learning_rate': 0.001069772649119388, 'subsample': 0.9861059582774021, 'colsample_bytree': 0.6257322247983843, 'reg_lambda': 0.2950283529769756}. Best is trial 11 with value: 0.8405989621214335.\n",
      "[I 2023-09-19 15:43:19,482] Trial 13 finished with value: 0.6091124489832687 and parameters: {'lags': 35, 'n_estimators': 308, 'max_depth': 6, 'min_child_weight': 2.6988898210583123, 'gamma': 0.02935294095467175, 'learning_rate': 0.002994670109964691, 'subsample': 0.9474165991463414, 'colsample_bytree': 0.5061180421138186, 'reg_lambda': 4.763753046027593}. Best is trial 11 with value: 0.8405989621214335.\n",
      "[I 2023-09-19 15:43:19,964] Trial 14 finished with value: 0.8279201946610607 and parameters: {'lags': 42, 'n_estimators': 359, 'max_depth': 6, 'min_child_weight': 3.268284713099089, 'gamma': 0.1256850805258285, 'learning_rate': 0.0010276442971462623, 'subsample': 0.9317682358617618, 'colsample_bytree': 0.6719976412383916, 'reg_lambda': 0.24818041302756855}. Best is trial 11 with value: 0.8405989621214335.\n",
      "[I 2023-09-19 15:43:20,114] Trial 15 finished with value: 0.817755000792721 and parameters: {'lags': 42, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 1.8666324578121527, 'gamma': 0.017546586415584497, 'learning_rate': 0.0032343304539988182, 'subsample': 0.997545571603558, 'colsample_bytree': 0.5038574920410632, 'reg_lambda': 0.0010269245914485657}. Best is trial 11 with value: 0.8405989621214335.\n",
      "[I 2023-09-19 15:43:20,373] Trial 16 finished with value: 0.6948237159962449 and parameters: {'lags': 28, 'n_estimators': 248, 'max_depth': 5, 'min_child_weight': 2.362097390334026, 'gamma': 0.09012085524700325, 'learning_rate': 0.0023941088649280695, 'subsample': 0.9263161865016918, 'colsample_bytree': 0.5808661404681315, 'reg_lambda': 4.138854582894748}. Best is trial 11 with value: 0.8405989621214335.\n",
      "[I 2023-09-19 15:43:20,931] Trial 17 finished with value: 0.30673381504530056 and parameters: {'lags': 42, 'n_estimators': 437, 'max_depth': 6, 'min_child_weight': 3.2430288691151414, 'gamma': 0.01150675580389529, 'learning_rate': 0.005120435560415607, 'subsample': 0.9200479583999451, 'colsample_bytree': 0.6768658386481895, 'reg_lambda': 0.8741605568801701}. Best is trial 11 with value: 0.8405989621214335.\n",
      "[I 2023-09-19 15:43:21,347] Trial 18 finished with value: 0.5166710784052083 and parameters: {'lags': 28, 'n_estimators': 599, 'max_depth': 3, 'min_child_weight': 0.06612502150220134, 'gamma': 0.037434425392668766, 'learning_rate': 0.0020003426308780915, 'subsample': 0.9945976231120903, 'colsample_bytree': 0.5510303825102503, 'reg_lambda': 0.16574802323000873}. Best is trial 11 with value: 0.8405989621214335.\n",
      "[I 2023-09-19 15:43:21,601] Trial 19 finished with value: 0.8537706136901424 and parameters: {'lags': 35, 'n_estimators': 195, 'max_depth': 5, 'min_child_weight': 2.0120579027311982, 'gamma': 0.004133037348170746, 'learning_rate': 0.0017648808617812964, 'subsample': 0.8144303458244111, 'colsample_bytree': 0.6432445647169998, 'reg_lambda': 0.5530873325444274}. Best is trial 19 with value: 0.8537706136901424.\n"
     ]
    }
   ],
   "source": [
    "# begin optimization\n",
    "#optuna.logging.set_verbosity(optuna.logging.WARNING) # won't print progress for every single trial\n",
    "study = optuna.create_study(directions=['maximize'])\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe which are the best parameters that have been obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lags': 35,\n",
       " 'n_estimators': 195,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 2.0120579027311982,\n",
       " 'gamma': 0.004133037348170746,\n",
       " 'learning_rate': 0.0017648808617812964,\n",
       " 'subsample': 0.8144303458244111,\n",
       " 'colsample_bytree': 0.6432445647169998,\n",
       " 'reg_lambda': 0.5530873325444274}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_trial.params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8537706136901424"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want we can also minimize our function by passing the parameter `direction='minimize'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-19 15:45:07,667] A new study created in memory with name: no-name-a93d3f8e-8d97-4966-9037-0781c6f20bc3\n",
      "[I 2023-09-19 15:45:08,258] Trial 0 finished with value: 0.3923412045391076 and parameters: {'lags': 49, 'n_estimators': 654, 'max_depth': 5, 'min_child_weight': 4.608809928648014, 'gamma': 0.14677908331244996, 'learning_rate': 0.06883945998410401, 'subsample': 0.5008153054468771, 'colsample_bytree': 0.7292802265885936, 'reg_lambda': 0.03542846872595663}. Best is trial 0 with value: 0.3923412045391076.\n",
      "[I 2023-09-19 15:45:08,361] Trial 1 finished with value: 0.5272550869750282 and parameters: {'lags': 28, 'n_estimators': 90, 'max_depth': 2, 'min_child_weight': 4.69818002249028, 'gamma': 0.05193291872225416, 'learning_rate': 0.017148826428957712, 'subsample': 0.8951282684657649, 'colsample_bytree': 0.6296805459514287, 'reg_lambda': 0.003014027615359843}. Best is trial 0 with value: 0.3923412045391076.\n",
      "[I 2023-09-19 15:45:08,952] Trial 2 finished with value: 0.43983091713950084 and parameters: {'lags': 28, 'n_estimators': 616, 'max_depth': 6, 'min_child_weight': 4.991143674091349, 'gamma': 0.004865361323935967, 'learning_rate': 0.06877114316318816, 'subsample': 0.7775273674381145, 'colsample_bytree': 0.5012095942080064, 'reg_lambda': 0.005544635690717517}. Best is trial 0 with value: 0.3923412045391076.\n",
      "[I 2023-09-19 15:45:09,060] Trial 3 finished with value: 0.339860049205573 and parameters: {'lags': 56, 'n_estimators': 84, 'max_depth': 2, 'min_child_weight': 4.874932128838354, 'gamma': 0.006664593406339509, 'learning_rate': 0.04081046534311997, 'subsample': 0.5772703863112632, 'colsample_bytree': 0.8108157963639319, 'reg_lambda': 0.03292645716561314}. Best is trial 3 with value: 0.339860049205573.\n",
      "[I 2023-09-19 15:45:09,386] Trial 4 finished with value: 0.45883698443452897 and parameters: {'lags': 42, 'n_estimators': 514, 'max_depth': 3, 'min_child_weight': 2.374874755565788, 'gamma': 0.9743661814778773, 'learning_rate': 0.04090129097319839, 'subsample': 0.9664235088285291, 'colsample_bytree': 0.9302488243435982, 'reg_lambda': 1.1998695164580913}. Best is trial 3 with value: 0.339860049205573.\n",
      "[I 2023-09-19 15:45:09,529] Trial 5 finished with value: 0.32903557595966226 and parameters: {'lags': 42, 'n_estimators': 104, 'max_depth': 4, 'min_child_weight': 2.3385987834811486, 'gamma': 0.05895156251958749, 'learning_rate': 0.029108435410713195, 'subsample': 0.6485095536950942, 'colsample_bytree': 0.6718468328740876, 'reg_lambda': 0.01205889127181224}. Best is trial 5 with value: 0.32903557595966226.\n",
      "[I 2023-09-19 15:45:09,738] Trial 6 finished with value: 0.42701081958561155 and parameters: {'lags': 14, 'n_estimators': 176, 'max_depth': 6, 'min_child_weight': 5.224714279704686, 'gamma': 0.015219411645323786, 'learning_rate': 0.057166969989805305, 'subsample': 0.5737180268925399, 'colsample_bytree': 0.5206863420263083, 'reg_lambda': 4.392889936346177}. Best is trial 5 with value: 0.32903557595966226.\n",
      "[I 2023-09-19 15:45:09,960] Trial 7 finished with value: 0.4632671651957596 and parameters: {'lags': 21, 'n_estimators': 301, 'max_depth': 3, 'min_child_weight': 1.345710879981634, 'gamma': 0.018441801815288456, 'learning_rate': 0.05139347726800863, 'subsample': 0.6023665567736463, 'colsample_bytree': 0.6012840793251344, 'reg_lambda': 0.152572941345956}. Best is trial 5 with value: 0.32903557595966226.\n",
      "[I 2023-09-19 15:45:10,193] Trial 8 finished with value: 0.5801350564690277 and parameters: {'lags': 21, 'n_estimators': 424, 'max_depth': 2, 'min_child_weight': 1.6122103578682894, 'gamma': 0.01652270703039942, 'learning_rate': 0.0033591669393712095, 'subsample': 0.6095247445383479, 'colsample_bytree': 0.8553453301016734, 'reg_lambda': 0.17817444009086988}. Best is trial 5 with value: 0.32903557595966226.\n",
      "[I 2023-09-19 15:45:10,527] Trial 9 finished with value: 0.6189456925632941 and parameters: {'lags': 14, 'n_estimators': 397, 'max_depth': 4, 'min_child_weight': 1.074105056950326, 'gamma': 0.01493892294600248, 'learning_rate': 0.0028388428738090987, 'subsample': 0.5590459974094946, 'colsample_bytree': 0.9982552821652579, 'reg_lambda': 0.08083389320850813}. Best is trial 5 with value: 0.32903557595966226.\n",
      "[I 2023-09-19 15:45:11,304] Trial 10 finished with value: 0.37098375388536625 and parameters: {'lags': 42, 'n_estimators': 923, 'max_depth': 4, 'min_child_weight': 0.03893408797426812, 'gamma': 0.0010903845777823406, 'learning_rate': 0.22621772825863823, 'subsample': 0.6893161536287247, 'colsample_bytree': 0.7106518989440342, 'reg_lambda': 0.0015055094925891085}. Best is trial 5 with value: 0.32903557595966226.\n",
      "[I 2023-09-19 15:45:11,518] Trial 11 finished with value: 0.29758389068783553 and parameters: {'lags': 56, 'n_estimators': 199, 'max_depth': 3, 'min_child_weight': 3.457594831335272, 'gamma': 0.2576051881247408, 'learning_rate': 0.013443510392006208, 'subsample': 0.6820945244904718, 'colsample_bytree': 0.8071947270514421, 'reg_lambda': 0.016800935015090893}. Best is trial 11 with value: 0.29758389068783553.\n",
      "[I 2023-09-19 15:45:11,831] Trial 12 finished with value: 0.27446151209759034 and parameters: {'lags': 56, 'n_estimators': 270, 'max_depth': 3, 'min_child_weight': 3.37279903829989, 'gamma': 0.4128699413758573, 'learning_rate': 0.00813196288011915, 'subsample': 0.70885138767339, 'colsample_bytree': 0.7953562005371594, 'reg_lambda': 0.011815059048900754}. Best is trial 12 with value: 0.27446151209759034.\n",
      "[I 2023-09-19 15:45:12,079] Trial 13 finished with value: 0.3629832017287085 and parameters: {'lags': 56, 'n_estimators': 270, 'max_depth': 3, 'min_child_weight': 3.62106920773125, 'gamma': 0.8762190978449674, 'learning_rate': 0.007589744721463985, 'subsample': 0.7386970859904336, 'colsample_bytree': 0.8058321908740907, 'reg_lambda': 0.008434433770544307}. Best is trial 12 with value: 0.27446151209759034.\n",
      "[I 2023-09-19 15:45:12,311] Trial 14 finished with value: 0.2778421629410049 and parameters: {'lags': 56, 'n_estimators': 264, 'max_depth': 3, 'min_child_weight': 3.5672497423799427, 'gamma': 5.301283556519115, 'learning_rate': 0.00849056588064565, 'subsample': 0.7720955222309098, 'colsample_bytree': 0.7762230044624515, 'reg_lambda': 0.0015124918138112466}. Best is trial 12 with value: 0.27446151209759034.\n",
      "[I 2023-09-19 15:45:13,328] Trial 15 finished with value: 0.591446947293311 and parameters: {'lags': 49, 'n_estimators': 800, 'max_depth': 5, 'min_child_weight': 3.995976617244076, 'gamma': 5.468623297920797, 'learning_rate': 0.0012693683269773349, 'subsample': 0.8001509543851522, 'colsample_bytree': 0.7586757075220939, 'reg_lambda': 0.0011699833572284069}. Best is trial 12 with value: 0.27446151209759034.\n",
      "[I 2023-09-19 15:45:13,607] Trial 16 finished with value: 0.3055831763788684 and parameters: {'lags': 49, 'n_estimators': 354, 'max_depth': 3, 'min_child_weight': 3.051533496507266, 'gamma': 2.902801033183999, 'learning_rate': 0.007227896928257766, 'subsample': 0.8184996171969458, 'colsample_bytree': 0.8812546959561444, 'reg_lambda': 0.003196752684663715}. Best is trial 12 with value: 0.27446151209759034.\n",
      "[I 2023-09-19 15:45:14,156] Trial 17 finished with value: 0.4014027957912855 and parameters: {'lags': 35, 'n_estimators': 491, 'max_depth': 5, 'min_child_weight': 5.679608021339067, 'gamma': 1.6136594079550513, 'learning_rate': 0.008554981396220471, 'subsample': 0.7331210259738963, 'colsample_bytree': 0.7482786949953754, 'reg_lambda': 0.0013112667879434424}. Best is trial 12 with value: 0.27446151209759034.\n",
      "[I 2023-09-19 15:45:14,421] Trial 18 finished with value: 0.5897065006226092 and parameters: {'lags': 56, 'n_estimators': 255, 'max_depth': 4, 'min_child_weight': 4.0759424311698655, 'gamma': 0.5391403133355306, 'learning_rate': 0.0038374045465109246, 'subsample': 0.8372962315971341, 'colsample_bytree': 0.7775543618891313, 'reg_lambda': 0.005228870401311421}. Best is trial 12 with value: 0.27446151209759034.\n",
      "[I 2023-09-19 15:45:14,719] Trial 19 finished with value: 0.7439782943844684 and parameters: {'lags': 49, 'n_estimators': 424, 'max_depth': 2, 'min_child_weight': 2.835951816664352, 'gamma': 4.876153354491311, 'learning_rate': 0.0010130623983973356, 'subsample': 0.7270519285510302, 'colsample_bytree': 0.7022487655097509, 'reg_lambda': 0.0032691168815424786}. Best is trial 12 with value: 0.27446151209759034.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe which are the best parameters that have been obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lags': 56,\n",
       " 'n_estimators': 270,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 3.37279903829989,\n",
       " 'gamma': 0.4128699413758573,\n",
       " 'learning_rate': 0.00813196288011915,\n",
       " 'subsample': 0.70885138767339,\n",
       " 'colsample_bytree': 0.7953562005371594,\n",
       " 'reg_lambda': 0.011815059048900754}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train our model with the best parameters obtained from the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_param = [XGBRegressor(**best_params) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = MLForecast(models=mejor_param,\n",
    "                    freq='W',\n",
    "                    lags=[28],\n",
    "                    lag_transforms={1: [expanding_mean], 12: [(rolling_mean, 7)] },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:45:29] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1693020860993/work/src/learner.cc:767: \n",
      "Parameters: { \"lags\" } are not used.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[XGBRegressor], freq=<Week: weekday=6>, lag_features=['lag28', 'expanding_mean_lag1', 'rolling_mean_lag12_window_size7'], date_features=[], num_threads=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>XGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>136.666107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>206.950119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>201.778046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>202.578064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>205.729736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>183.612137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-13</td>\n",
       "      <td>172.488846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-20</td>\n",
       "      <td>227.618134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>225.309097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>235.971085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>231.503281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>231.228714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>206.249329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>148.997559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>229.750259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>238.120422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>215.677734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>241.779907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>202.084854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>219.726471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>210.397415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-29</td>\n",
       "      <td>248.285828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-05</td>\n",
       "      <td>211.757706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>203.184540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-19</td>\n",
       "      <td>195.601807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-26</td>\n",
       "      <td>173.735474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-03</td>\n",
       "      <td>215.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-10</td>\n",
       "      <td>200.695709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>190.196823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>180.168762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds  XGBRegressor\n",
       "0          1 2023-01-02    136.666107\n",
       "1          1 2023-01-09    206.950119\n",
       "2          1 2023-01-16    201.778046\n",
       "3          1 2023-01-23    202.578064\n",
       "4          1 2023-01-30    205.729736\n",
       "5          1 2023-02-06    183.612137\n",
       "6          1 2023-02-13    172.488846\n",
       "7          1 2023-02-20    227.618134\n",
       "8          1 2023-02-27    225.309097\n",
       "9          1 2023-03-06    235.971085\n",
       "10         1 2023-03-13    231.503281\n",
       "11         1 2023-03-20    231.228714\n",
       "12         1 2023-03-27    206.249329\n",
       "13         1 2023-04-03    148.997559\n",
       "14         1 2023-04-10    229.750259\n",
       "15         1 2023-04-17    238.120422\n",
       "16         1 2023-04-24    215.677734\n",
       "17         1 2023-05-01    241.779907\n",
       "18         1 2023-05-08    202.084854\n",
       "19         1 2023-05-15    219.726471\n",
       "20         1 2023-05-22    210.397415\n",
       "21         1 2023-05-29    248.285828\n",
       "22         1 2023-06-05    211.757706\n",
       "23         1 2023-06-12    203.184540\n",
       "24         1 2023-06-19    195.601807\n",
       "25         1 2023-06-26    173.735474\n",
       "26         1 2023-07-03    215.806900\n",
       "27         1 2023-07-10    200.695709\n",
       "28         1 2023-07-17    190.196823\n",
       "29         1 2023-07-24    180.168762"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_df = mlf.predict(h=30) \n",
    "forecast_df[\"ds\"]=forecast_df[\"ds\"]+timedelta(days=1)\n",
    "forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plot_series(df, forecast_df, max_insample_length=5000,engine=\"matplotlib\")\n",
    "for ax in fig.get_axes():\n",
    "   ax.set_title(\"Prediction\")\n",
    "fig.savefig('../figs/hiperparameters__plot_best_model.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/hiperparameters__plot_best_model.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **References** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "[Table of Contents](#0)\n",
    "\n",
    "1. Changquan Huang • Alla Petukhina. Springer series (2022). Applied Time Series Analysis and Forecasting with Python. \n",
    "2. Ivan Svetunkov. [Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)](https://openforecast.org/adam/)\n",
    "3. [James D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.](https://press.princeton.edu/books/hardcover/9780691042893/time-series-analysis)\n",
    "4. [Nixtla Parameters for Mlforecast](https://nixtla.github.io/mlforecast/forecast.html).\n",
    "5. [Pandas available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "6. [Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Time series cross-validation”.](https://otexts.com/fpp3/tscv.html).\n",
    "7. [Seasonal periods- Rob J Hyndman](https://robjhyndman.com/hyndsight/seasonal-periods/).\n",
    "8. [Optuna](https://optuna.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlforecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
